{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "95cb8a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2172e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# points in face:\n",
    "# nose[0]\n",
    "# l_eye_outer[3]\n",
    "# r_eye_outer[6]\n",
    "# mouth_left[9]\n",
    "# mouth_right[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a10e9da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "863ec8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "with mp_pose.Pose(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as pose:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    height, width, _ = image.shape\n",
    "    size = image.shape\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # Flip the image horizontally for a later selfie-view display, and convert\n",
    "    # the BGR image to RGB.\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    results = pose.process(image)\n",
    "\n",
    "    face_point_x = []\n",
    "    face_point_y = []\n",
    "    for i in range(11):\n",
    "        face_point_x.append(results.pose_landmarks.landmark[i].x * width)\n",
    "        face_point_y.append(results.pose_landmarks.landmark[i].y * height)\n",
    "    face_point = list(zip(face_point_x, face_point_y))\n",
    "    \n",
    "    image_points = np.array([\n",
    "        face_point[0],     # Nose tip\n",
    "\n",
    "        face_point[3],     # Left eye left corner\n",
    "\n",
    "        face_point[6],     # Right eye right corne\n",
    "\n",
    "        face_point[9],     # Left Mouth corner\n",
    "\n",
    "        face_point[10]      # Right mouth corner\n",
    "    ], dtype=\"double\")\n",
    "    \n",
    "    # 3D model points.\n",
    "\n",
    "    model_points = np.array([\n",
    "    \n",
    "        (0.0, 0.0, 0.0),             # Nose tip\n",
    "    \n",
    "        (-225.0, 170.0, -135.0),     # Left eye left corner\n",
    "    \n",
    "        (225.0, 170.0, -135.0),      # Right eye right corne\n",
    "    \n",
    "        (-150.0, -150.0, -125.0),    # Left Mouth corner\n",
    "    \n",
    "        (150.0, -150.0, -125.0)      # Right mouth corner\n",
    "    ])\n",
    "    \n",
    "    # Camera internals\n",
    "\n",
    "\n",
    "\n",
    "    focal_length = size[1]\n",
    "\n",
    "    center = (size[1]/2, size[0]/2)\n",
    "\n",
    "    camera_matrix = np.array(\n",
    "\n",
    "                             [[focal_length, 0, center[0]],\n",
    "\n",
    "                             [0, focal_length, center[1]],\n",
    "\n",
    "                             [0, 0, 1]], dtype = \"double\"\n",
    "\n",
    "                             )\n",
    "\n",
    "    dist_coeffs = np.zeros((4,1)) # Assuming no lens distortion\n",
    "    (success, rotation_vector, translation_vector) = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_UPNP)\n",
    "    (nose_end_point2D, jacobian) = cv2.projectPoints(np.array([(0.0, 0.0, 1000.0)]), rotation_vector, translation_vector, camera_matrix, dist_coeffs)\n",
    "\n",
    "\n",
    "\n",
    "    for p in image_points:\n",
    "        cv2.circle(image, (int(p[0]), int(p[1])), 3, (0,0,255), -1)\n",
    "\n",
    "    p1 = ( int(image_points[0][0]), int(image_points[0][1]))\n",
    "    p2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]))\n",
    "\n",
    "    cv2.line(image, p1, p2, (255,0,0), 2)\n",
    "    \n",
    "#     for point in face_point:\n",
    "#         cv2.circle(image, (int(point[0]), int(point[1])), 3, (0,0,255), -1)\n",
    "\n",
    "    # Draw the pose annotation on the image.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "#     mp_drawing.draw_landmarks(\n",
    "#         image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    cv2.imshow('MediaPipe Pose', image)\n",
    "    if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "      break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93816937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_annotation_box(img, rotation_vector, translation_vector, camera_matrix, color=(255, 255, 0), line_width=2):\n",
    "    \"\"\"Draw a 3D box as annotation of pose\"\"\"\n",
    "    point_3d = []\n",
    "    dist_coeffs = np.zeros((4,1))\n",
    "    rear_size = 1\n",
    "    rear_depth = 0\n",
    "    point_3d.append((-rear_size, -rear_size, rear_depth))\n",
    "    point_3d.append((-rear_size, rear_size, rear_depth))\n",
    "    point_3d.append((rear_size, rear_size, rear_depth))\n",
    "    point_3d.append((rear_size, -rear_size, rear_depth))\n",
    "    point_3d.append((-rear_size, -rear_size, rear_depth))\n",
    "\n",
    "    front_size = img.shape[1]\n",
    "    front_depth = front_size*2\n",
    "    point_3d.append((-front_size, -front_size, front_depth))\n",
    "    point_3d.append((-front_size, front_size, front_depth))\n",
    "    point_3d.append((front_size, front_size, front_depth))\n",
    "    point_3d.append((front_size, -front_size, front_depth))\n",
    "    point_3d.append((-front_size, -front_size, front_depth))\n",
    "    point_3d = np.array(point_3d, dtype=np.float32).reshape(-1, 3)\n",
    "\n",
    "    # Map to 2d img points\n",
    "    (point_2d, _) = cv2.projectPoints(point_3d,\n",
    "                                      rotation_vector,\n",
    "                                      translation_vector,\n",
    "                                      camera_matrix,\n",
    "                                      dist_coeffs)\n",
    "    point_2d = np.int32(point_2d.reshape(-1, 2))\n",
    "    \n",
    "\n",
    "    # # Draw all the lines\n",
    "    # cv2.polylines(img, [point_2d], True, color, line_width, cv2.LINE_AA)\n",
    "    k = (point_2d[5] + point_2d[8])//2\n",
    "    # cv2.line(img, tuple(point_2d[1]), tuple(\n",
    "    #     point_2d[6]), color, line_width, cv2.LINE_AA)\n",
    "    # cv2.line(img, tuple(point_2d[2]), tuple(\n",
    "    #     point_2d[7]), color, line_width, cv2.LINE_AA)\n",
    "    # cv2.line(img, tuple(point_2d[3]), tuple(\n",
    "    #     point_2d[8]), color, line_width, cv2.LINE_AA)\n",
    "    \n",
    "    return(point_2d[2], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4566331b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-116-0f55ee417338>:119: RuntimeWarning: divide by zero encountered in int_scalars\n",
      "  m = (x2[1] - x1[1])/(x2[0] - x1[0])\n"
     ]
    }
   ],
   "source": [
    "# landmark in holistic:\n",
    "# nose_tip = [1]\n",
    "# chin = [152]\n",
    "# l_eye_corner = [130]\n",
    "# r_eye_corner = [359]\n",
    "# l_mouth = [61]\n",
    "# r_mouth = [291]\n",
    "index_list = [1, 152, 130, 359, 61, 291]\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "#cap = cv2.VideoCapture(\"../media/video/face_me.mp4\")\n",
    "\n",
    "with mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "\n",
    "        success, image = cap.read()\n",
    "        height, width, _ = image.shape\n",
    "        size = image.shape\n",
    "        if not success:\n",
    "          print(\"Ignoring empty camera frame.\")\n",
    "          # If loading a video, use 'break' instead of 'continue'.\n",
    "          continue\n",
    "\n",
    "        # Flip the image horizontally for a later selfie-view display, and convert\n",
    "        # the BGR image to RGB.\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        try:\n",
    "            results = holistic.process(image)\n",
    "\n",
    "            face_point_x = []\n",
    "            face_point_y = []\n",
    "            for i in index_list:\n",
    "                face_point_x.append(results.face_landmarks.landmark[i].x * width)\n",
    "                face_point_y.append(results.face_landmarks.landmark[i].y * height)\n",
    "            face_point = list(zip(face_point_x, face_point_y))\n",
    "\n",
    "            image_points = np.array([\n",
    "                face_point[0],     # Nose tip\n",
    "\n",
    "                face_point[1],     # Chin\n",
    "\n",
    "                face_point[2],     # Left eye left corner\n",
    "\n",
    "                face_point[3],     # Right eye right corne\n",
    "\n",
    "                face_point[4],     # Left Mouth corner\n",
    "\n",
    "                face_point[5]      # Right mouth corner\n",
    "            ], dtype=\"double\")\n",
    "\n",
    "            # 3D model points.\n",
    "\n",
    "            model_points = np.array([\n",
    "\n",
    "                (0.0, 0.0, 0.0),             # Nose tip\n",
    "\n",
    "                (0.0, -330.0, -65.0),        # Chin\n",
    "\n",
    "                (-225.0, 170.0, -135.0),     # Left eye left corner\n",
    "\n",
    "                (225.0, 170.0, -135.0),      # Right eye right corne\n",
    "\n",
    "                (-150.0, -150.0, -125.0),    # Left Mouth corner\n",
    "\n",
    "                (150.0, -150.0, -125.0)      # Right mouth corner\n",
    "            ])\n",
    "\n",
    "            # Camera internals\n",
    "\n",
    "\n",
    "\n",
    "            focal_length = size[1]\n",
    "\n",
    "            center = (size[1]/2, size[0]/2)\n",
    "\n",
    "            camera_matrix = np.array(\n",
    "\n",
    "                                     [[focal_length, 0, center[0]],\n",
    "\n",
    "                                     [0, focal_length, center[1]],\n",
    "\n",
    "                                     [0, 0, 1]], dtype = \"double\"\n",
    "\n",
    "                                     )\n",
    "\n",
    "            dist_coeffs = np.zeros((4,1)) # Assuming no lens distortion\n",
    "            (success, rotation_vector, translation_vector) = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_UPNP)\n",
    "            (nose_end_point2D, jacobian) = cv2.projectPoints(np.array([(0.0, 0.0, 1000.0)]), rotation_vector, translation_vector, camera_matrix, dist_coeffs)\n",
    "\n",
    "\n",
    "\n",
    "            for p in image_points:\n",
    "                cv2.circle(image, (int(p[0]), int(p[1])), 3, (0,0,255), -1)\n",
    "\n",
    "            p1 = ( int(image_points[0][0]), int(image_points[0][1]))\n",
    "            p2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]))\n",
    "            x1, x2 = draw_annotation_box(image, rotation_vector, translation_vector, camera_matrix)\n",
    "\n",
    "            cv2.line(image, p1, p2, (255,0,0), 2)\n",
    "            cv2.line(image, (0, int(face_point[0][1])), (width, int(face_point[0][1])), (255, 0, 0), 2)\n",
    "            if p2[1] > p1[1] + 50:\n",
    "                cv2.putText(image, \"LOOKING DOWN\", (20, 100), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255))\n",
    "            #print(face_point[0][1])\n",
    "            try:\n",
    "                m = (p2[1] - p1[1])/(p2[0] - p1[0])\n",
    "                ang1 = int(math.degrees(math.atan(m)))\n",
    "            except:\n",
    "                ang1 = 90\n",
    "\n",
    "            try:\n",
    "                m = (x2[1] - x1[1])/(x2[0] - x1[0])\n",
    "                ang2 = int(math.degrees(math.atan(-1/m)))\n",
    "            except:\n",
    "                ang2 = 90\n",
    "\n",
    "                    # print('div by zero error')\n",
    "            #cv2.putText(image, str(ang1), tuple(p1), font, 2, (128, 255, 255), 3)\n",
    "            cv2.putText(image, str(ang2), tuple(x1), font, 2, (255, 255, 128), 3)\n",
    "\n",
    "        #     for point in face_point:\n",
    "        #         cv2.circle(image, (int(point[0]), int(point[1])), 3, (0,0,255), -1)\n",
    "\n",
    "            # Draw the pose annotation on the image.\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        #     mp_drawing.draw_landmarks(\n",
    "        #         image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        except:\n",
    "            cv2.putText(image, \"NO DETECTION\", (20, 100), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255))\n",
    "            pass\n",
    "        cv2.imshow('MediaPipe Pose', image)\n",
    "        if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "          break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

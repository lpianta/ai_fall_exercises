{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "given-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "advanced-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regular-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of landmarks in right and left eye\n",
    "# horizontal line for left eye: [0, 8]\n",
    "# first vertical line: [3, 13]\n",
    "# second vertical line: [5, 11]\n",
    "left_eye_lm = [33, 246, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 145, 144, 163, 7]\n",
    "# horizontal line for right eye: [0, 8]\n",
    "# first vertical line: [3, 13]\n",
    "# second vertical line: [5, 11]\n",
    "right_eye_lm = [263, 466, 388, 387, 386, 385, 384, 398, 362, 382, 381, 380, 374, 373, 390, 249]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stock-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_coords(frame, landmarks):\n",
    "    height, width, _ = frame.shape\n",
    "    eye_x = []\n",
    "    eye_y = []\n",
    "    for lm in landmarks:\n",
    "        # mp return normalized coords, so we multiply\n",
    "        eye_x.append(int((results.face_landmarks.landmark[lm].x) * width))\n",
    "        eye_y.append(int((results.face_landmarks.landmark[lm].y) * height))\n",
    "    eye = list(zip(eye_x, eye_y))\n",
    "    eye = np.array(eye, dtype=\"int\")\n",
    "    return eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "optical-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "    # euclidean distances between the two sets of vertical eye landmarks (x, y)-coordinates\n",
    "    A = dist.euclidean(eye[3], eye[13])\n",
    "    B = dist.euclidean(eye[5], eye[11])\n",
    "    # euclidean distance between the horizontal eye landmark (x, y)-coordinates\n",
    "    C = dist.euclidean(eye[0], eye[8])\n",
    "    # eye aspect ratio\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "smart-shareware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EAR 0.2502786234416282\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.23038929263524183\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.24662507289577973\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.2179394105221504\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.25060795380341494\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.23979588463151813\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.2169935752372592\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.23135275489669435\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.24159119430725728\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.2524337224835045\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.2169935752372592\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.24116765286036163\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.22091234211683286\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.23653627048349485\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.24123846673071114\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.22147743402046327\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.23161564386160075\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.24202901516487996\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.2047040840543957\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.24041465641929932\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.22673050646408915\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.22639029738529767\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.22571361543846513\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.22506696413934374\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.24213887580051222\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.2255997489511289\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.22328537899596096\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n",
      "EAR 0.22328537899596096\n",
      "THRESH 0.2574301854727729\n",
      "drowsy\n"
     ]
    }
   ],
   "source": [
    "# initialize counters and threshold\n",
    "frame_counter = 0\n",
    "ear_frame_counter = 0\n",
    "ear_threshold = []\n",
    "\n",
    "#cap = cv2.VideoCapture(\"../media/video/face_7min2.mp4\")\n",
    "cap = cv2.VideoCapture(1)\n",
    "#cap = cv2.VideoCapture(\"http://192.168.2.117:8080/video\")\n",
    "\n",
    "with mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # update frame counter\n",
    "        #frame_counter += 1\n",
    "        frame = cv2.resize(frame, (640, 480))\n",
    "        #frame = cv2.flip(frame, 0)\n",
    "        #frame = cv2.flip(frame, 1)\n",
    "\n",
    "\n",
    "        height, width, _ = frame.shape\n",
    "        # Convert the BGR image to RGB.\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        frame.flags.writeable = False\n",
    "        \n",
    "        try:\n",
    "            results = holistic.process(frame)\n",
    "            \n",
    "            if results.face_landmarks:\n",
    "                frame_counter += 1\n",
    "            \n",
    "            # get eyes coords\n",
    "            left_eye = eye_coords(frame, left_eye_lm)\n",
    "            right_eye = eye_coords(frame, right_eye_lm)\n",
    "\n",
    "            # get EARs\n",
    "            left_ear = eye_aspect_ratio(left_eye)\n",
    "            right_ear = eye_aspect_ratio(right_eye)\n",
    "\n",
    "            # average of both EARs\n",
    "            ear = (left_ear + right_ear) / 2\n",
    "\n",
    "            # find convex hull for the eyes   \n",
    "            leye_hull = cv2.convexHull(left_eye)\n",
    "            reye_hull = cv2.convexHull(right_eye)\n",
    "\n",
    "            # Draw convex hull\n",
    "            frame.flags.writeable = True\n",
    "            cv2.drawContours(frame, [leye_hull], -1, (0, 255, 0), 1)\n",
    "            cv2.drawContours(frame, [reye_hull], -1, (0, 255, 0), 1)\n",
    "\n",
    "            # take some time to initialize values\n",
    "            if frame_counter < 30:\n",
    "                cv2.putText(frame, \"CALIBRATING\", (20, 20), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255))\n",
    "                ear_threshold.append(ear)\n",
    "            else:\n",
    "                # convert ear_threshold to numpy array\n",
    "                ear_threshold = np.array(ear_threshold)\n",
    "               # remove NaN values from the array\n",
    "                ear_threshold = ear_threshold[~np.isnan(ear_threshold)] # ~ = is not\n",
    "                # get the mean\n",
    "                ear_threshold = np.mean(np.array(ear_threshold))\n",
    "\n",
    "                # print EAR on the frame\n",
    "                cv2.putText(frame, f\"EAR: {round(ear, 2)}\", (20, 20), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255))\n",
    "\n",
    "                # check if EAR goes below the threshold for a number of frames\n",
    "                if ear < ear_threshold:\n",
    "                    ear_frame_counter += 1\n",
    "\n",
    "                    if ear_frame_counter >= 10:\n",
    "                        print(\"EAR \" + str(ear))\n",
    "                        print(\"THRESH \" + str(ear_threshold))\n",
    "                        print(\"drowsy\")\n",
    "                        cv2.putText(frame, \"WARNING!\", (20, 100), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255))\n",
    "\n",
    "                else:\n",
    "                    ear_frame_counter = 0\n",
    "        except:\n",
    "            cv2.putText(frame, \"NO DETECTION\", (20, 100), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255))\n",
    "            pass\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        \n",
    "        cv2.imshow('Webcam Feed', frame)\n",
    "\n",
    "        if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

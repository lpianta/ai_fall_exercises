{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "psychological-alarm",
   "metadata": {},
   "source": [
    "## Importing Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unnecessary-pressing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:42.520625Z",
     "start_time": "2021-03-28T20:32:42.518920Z"
    }
   },
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "## I recommend the one above, because the following is more accurate but less efficient\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "frank-monaco",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:25.501688Z",
     "start_time": "2021-03-31T09:15:24.378850Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# You can also load en_core_web_lg that has an higher accuracy but it's less efficient\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brutal-usage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:43.994556Z",
     "start_time": "2021-03-28T20:32:43.991087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f86ff1ef9f0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f86ff19f950>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f86ff1b2100>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f86ff3b3be0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f86ff1b4cc0>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7f86ff161440>)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "marine-rescue",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.015363Z",
     "start_time": "2021-03-28T20:32:43.995410Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process sentences 'Hello, world. Antonio is learning Python.' using spaCy\n",
    "doc = nlp(u\"Hello, world. Antonio is learning Python.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "behavioral-prospect",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.021660Z",
     "start_time": "2021-03-28T20:32:44.016337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "world\n",
      ".\n",
      "Antonio\n",
      "is\n",
      "learning\n",
      "Python\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fleet-shooting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.030363Z",
     "start_time": "2021-03-28T20:32:44.022649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello, world.\n",
      "Antonio is learning Python.\n"
     ]
    }
   ],
   "source": [
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "phantom-intranet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.043811Z",
     "start_time": "2021-03-28T20:32:44.033898Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = nlp(\"Let's go to N.Y.!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "designed-capitol",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.049403Z",
     "start_time": "2021-03-28T20:32:44.045680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-duplicate",
   "metadata": {},
   "source": [
    "As you have seen, using `nlp`, that comes from `spacy.load(\"en_core_web_sm\")`, you get the tokenized version of the sentence. If you want only the instance of the `Tokenizer` class, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accomplished-marker",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.054850Z",
     "start_time": "2021-03-28T20:32:44.050350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokenizer.Tokenizer"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nlp.tokenizer\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-medium",
   "metadata": {},
   "source": [
    "If you want to instantiate a custom one, with rules and prefixes and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "olive-mother",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.062007Z",
     "start_time": "2021-03-28T20:32:44.055855Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(vocab=nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-clerk",
   "metadata": {},
   "source": [
    "The tokenizer defined above contains only english rules.\n",
    "Let's test it on \"Let's go to N.Y.!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "distinct-trade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.068307Z",
     "start_time": "2021-03-28T20:32:44.063416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's\n",
      "go\n",
      "to\n",
      "N.Y.!\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\"Let's go to N.Y.!\")\n",
    "for token in tokens:\n",
    "    print(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-county",
   "metadata": {},
   "source": [
    "As you can see here, it doesn't handle the exceptions about the dots. So we can add rules for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "opposed-confusion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.076430Z",
     "start_time": "2021-03-28T20:32:44.069248Z"
    }
   },
   "outputs": [],
   "source": [
    "prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "foster-highway",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.083023Z",
     "start_time": "2021-03-28T20:32:44.077855Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    vocab=nlp.vocab, prefix_search=prefix_re.search, suffix_search=suffix_re.search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adjacent-organ",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.088977Z",
     "start_time": "2021-03-28T20:32:44.083988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\"Let's go to N.Y.!\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-provincial",
   "metadata": {},
   "source": [
    "You can also check the exceptions the tokenizer can handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "exposed-compound",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.103258Z",
     "start_time": "2021-03-28T20:32:44.092331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([[{65: ' '}], [{65: '\\t'}], [{65: '\\\\t'}], [{65: '\\n'}], [{65: '\\\\n'}], [{65: '—'}], [{65: '\\xa0', 67: '  '}], [{65: \"'\"}], [{65: '\\\\\")'}], [{65: '<space>'}], [{65: \"''\"}], [{65: 'C++'}], [{65: 'a.'}], [{65: 'b.'}], [{65: 'c.'}], [{65: 'd.'}], [{65: 'e.'}], [{65: 'f.'}], [{65: 'g.'}], [{65: 'h.'}], [{65: 'i.'}], [{65: 'j.'}], [{65: 'k.'}], [{65: 'l.'}], [{65: 'm.'}], [{65: 'n.'}], [{65: 'o.'}], [{65: 'p.'}], [{65: 'q.'}], [{65: 'r.'}], [{65: 's.'}], [{65: 't.'}], [{65: 'u.'}], [{65: 'v.'}], [{65: 'w.'}], [{65: 'x.'}], [{65: 'y.'}], [{65: 'z.'}], [{65: 'ä.'}], [{65: 'ö.'}], [{65: 'ü.'}], [{65: '[:'}], [{65: \":'(\"}], [{65: '=3'}], [{65: '>.>'}], [{65: ':))'}], [{65: '(='}], [{65: ':X'}], [{65: 'O.O'}], [{65: '<333'}], [{65: '<33'}], [{65: ':-o'}], [{65: '=]'}], [{65: ':-('}], [{65: '(-;'}], [{65: '><(((*>'}], [{65: 'o.O'}], [{65: '(-:'}], [{65: 'O_O'}], [{65: 'o.0'}], [{65: '</3'}], [{65: '=['}], [{65: '=('}], [{65: ':-/'}], [{65: '0_0'}], [{65: 'V_V'}], [{65: ':-O'}], [{65: 'O_o'}], [{65: '=)'}], [{65: 'XD'}], [{65: ':D'}], [{65: 'O.o'}], [{65: ':*'}], [{65: ':()'}], [{65: 'o_O'}], [{65: '=|'}], [{65: ':}'}], [{65: ':>'}], [{65: ';D'}], [{65: ']='}], [{65: 'o_0'}], [{65: '^___^'}], [{65: '0.o'}], [{65: '(-8'}], [{65: '^_^'}], [{65: '(._.)'}], [{65: ':-((('}], [{65: ':3'}], [{65: '(-_-)'}], [{65: ':)'}], [{65: \":'-(\"}], [{65: '0_o'}], [{65: '>:('}], [{65: ':-D'}], [{65: ':O'}], [{65: ':((('}], [{65: '8-)'}], [{65: '-__-'}], [{65: ':-]'}], [{65: '(;'}], [{65: ':-p'}], [{65: 'XDD'}], [{65: '<.<'}], [{65: ':->'}], [{65: ':-)))'}], [{65: ':-*'}], [{65: ';-D'}], [{65: '=/'}], [{65: ':p'}], [{65: ';-)'}], [{65: '._.'}], [{65: ':-3'}], [{65: '¯\\\\(ツ)/¯'}], [{65: ':1'}], [{65: '):'}], [{65: \":')\"}], [{65: 'xD'}], [{65: '0.0'}], [{65: '(^_^)'}], [{65: '(o:'}], [{65: ':]'}], [{65: ':-}'}], [{65: ':(('}], [{65: 'v_v'}], [{65: 'ಠ︵ಠ'}], [{65: ':-)'}], [{65: '>.<'}], [{65: 'xDD'}], [{65: '@_@'}], [{65: ':P'}], [{65: '8-D'}], [{65: '[-:'}], [{65: ':0'}], [{65: ':-|'}], [{65: '(¬_¬)'}], [{65: ':-(('}], [{65: 'V.V'}], [{65: 'v.v'}], [{65: '(ಠ_ಠ)'}], [{65: ':o)'}], [{65: \":'-)\"}], [{65: '(*_*)'}], [{65: 'ಠ_ಠ'}], [{65: ':/'}], [{65: ':o'}], [{65: '(>_<)'}], [{65: ';_;'}], [{65: '[='}], [{65: ':-0'}], [{65: ':x'}], [{65: '(╯°□°）╯︵┻━┻'}], [{65: '<3'}], [{65: 'o_o'}], [{65: ')-:'}], [{65: '8D'}], [{65: '>:o'}], [{65: ':-X'}], [{65: ':-x'}], [{65: '(:'}], [{65: '^__^'}], [{65: ':)))'}], [{65: ':-))'}], [{65: '8)'}], [{65: '=D'}], [{65: ':|'}], [{65: ';)'}], [{65: 'o.o'}], [{65: ':('}], [{65: '-_-'}], [{65: ':-P'}], [{65: 'i', 67: 'i'}, {65: \"'m\", 67: 'am'}], [{65: 'i', 67: 'i'}, {65: 'm'}], [{65: 'i', 67: 'i'}, {65: \"'m\", 67: 'am'}, {65: 'a', 67: 'gonna'}], [{65: 'i', 67: 'i'}, {65: 'm', 67: 'am'}, {65: 'a', 67: 'gonna'}], [{65: 'I', 67: 'i'}, {65: \"'m\", 67: 'am'}], [{65: 'I', 67: 'i'}, {65: 'm'}], [{65: 'I', 67: 'i'}, {65: \"'m\", 67: 'am'}, {65: 'a', 67: 'gonna'}], [{65: 'I', 67: 'i'}, {65: 'm', 67: 'am'}, {65: 'a', 67: 'gonna'}], [{65: 'i', 67: 'i'}, {65: \"'ll\", 67: 'will'}], [{65: 'i', 67: 'i'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'i', 67: 'i'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'i', 67: 'i'}, {65: \"'d\", 67: \"'d\"}], [{65: 'i', 67: 'i'}, {65: 'd', 67: \"'d\"}], [{65: 'i', 67: 'i'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'i', 67: 'i'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'I', 67: 'i'}, {65: \"'ll\", 67: 'will'}], [{65: 'I', 67: 'i'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'I', 67: 'i'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'I', 67: 'i'}, {65: \"'d\", 67: \"'d\"}], [{65: 'I', 67: 'i'}, {65: 'd', 67: \"'d\"}], [{65: 'I', 67: 'i'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'I', 67: 'i'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: \"'ll\", 67: 'will'}], [{65: 'you', 67: 'you'}, {65: 'll', 67: 'will'}], [{65: 'you', 67: 'you'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'you', 67: 'you'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: \"'d\", 67: \"'d\"}], [{65: 'you', 67: 'you'}, {65: 'd', 67: \"'d\"}], [{65: 'you', 67: 'you'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'you', 67: 'you'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'You', 67: 'you'}, {65: \"'ll\", 67: 'will'}], [{65: 'You', 67: 'you'}, {65: 'll', 67: 'will'}], [{65: 'You', 67: 'you'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'You', 67: 'you'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'You', 67: 'you'}, {65: \"'d\", 67: \"'d\"}], [{65: 'You', 67: 'you'}, {65: 'd', 67: \"'d\"}], [{65: 'You', 67: 'you'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'You', 67: 'you'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'he', 67: 'he'}, {65: \"'ll\", 67: 'will'}], [{65: 'he', 67: 'he'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'he', 67: 'he'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'he', 67: 'he'}, {65: \"'d\", 67: \"'d\"}], [{65: 'he', 67: 'he'}, {65: 'd', 67: \"'d\"}], [{65: 'he', 67: 'he'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'he', 67: 'he'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'He', 67: 'he'}, {65: \"'ll\", 67: 'will'}], [{65: 'He', 67: 'he'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'He', 67: 'he'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'He', 67: 'he'}, {65: \"'d\", 67: \"'d\"}], [{65: 'He', 67: 'he'}, {65: 'd', 67: \"'d\"}], [{65: 'He', 67: 'he'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'He', 67: 'he'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'she', 67: 'she'}, {65: \"'ll\", 67: 'will'}], [{65: 'she', 67: 'she'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'she', 67: 'she'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'she', 67: 'she'}, {65: \"'d\", 67: \"'d\"}], [{65: 'she', 67: 'she'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'she', 67: 'she'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'She', 67: 'she'}, {65: \"'ll\", 67: 'will'}], [{65: 'She', 67: 'she'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'She', 67: 'she'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'She', 67: 'she'}, {65: \"'d\", 67: \"'d\"}], [{65: 'She', 67: 'she'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'She', 67: 'she'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'it', 67: 'it'}, {65: \"'ll\", 67: 'will'}], [{65: 'it', 67: 'it'}, {65: 'll', 67: 'will'}], [{65: 'it', 67: 'it'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'it', 67: 'it'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'it', 67: 'it'}, {65: \"'d\", 67: \"'d\"}], [{65: 'it', 67: 'it'}, {65: 'd', 67: \"'d\"}], [{65: 'it', 67: 'it'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'it', 67: 'it'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'It', 67: 'it'}, {65: \"'ll\", 67: 'will'}], [{65: 'It', 67: 'it'}, {65: 'll', 67: 'will'}], [{65: 'It', 67: 'it'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'It', 67: 'it'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'It', 67: 'it'}, {65: \"'d\", 67: \"'d\"}], [{65: 'It', 67: 'it'}, {65: 'd', 67: \"'d\"}], [{65: 'It', 67: 'it'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'It', 67: 'it'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'we', 67: 'we'}, {65: \"'ll\", 67: 'will'}], [{65: 'we', 67: 'we'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'we', 67: 'we'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'we', 67: 'we'}, {65: \"'d\", 67: \"'d\"}], [{65: 'we', 67: 'we'}, {65: 'd', 67: \"'d\"}], [{65: 'we', 67: 'we'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'we', 67: 'we'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'We', 67: 'we'}, {65: \"'ll\", 67: 'will'}], [{65: 'We', 67: 'we'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'We', 67: 'we'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'We', 67: 'we'}, {65: \"'d\", 67: \"'d\"}], [{65: 'We', 67: 'we'}, {65: 'd', 67: \"'d\"}], [{65: 'We', 67: 'we'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'We', 67: 'we'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'they', 67: 'they'}, {65: \"'ll\", 67: 'will'}], [{65: 'they', 67: 'they'}, {65: 'll', 67: 'will'}], [{65: 'they', 67: 'they'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'they', 67: 'they'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'they', 67: 'they'}, {65: \"'d\", 67: \"'d\"}], [{65: 'they', 67: 'they'}, {65: 'd', 67: \"'d\"}], [{65: 'they', 67: 'they'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'they', 67: 'they'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'They', 67: 'they'}, {65: \"'ll\", 67: 'will'}], [{65: 'They', 67: 'they'}, {65: 'll', 67: 'will'}], [{65: 'They', 67: 'they'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'They', 67: 'they'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'They', 67: 'they'}, {65: \"'d\", 67: \"'d\"}], [{65: 'They', 67: 'they'}, {65: 'd', 67: \"'d\"}], [{65: 'They', 67: 'they'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'They', 67: 'they'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'i', 67: 'i'}, {65: \"'ve\", 67: 'have'}], [{65: 'i', 67: 'i'}, {65: 've', 67: 'have'}], [{65: 'I', 67: 'i'}, {65: \"'ve\", 67: 'have'}], [{65: 'I', 67: 'i'}, {65: 've', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: \"'ve\", 67: 'have'}], [{65: 'you', 67: 'you'}, {65: 've', 67: 'have'}], [{65: 'You', 67: 'you'}, {65: \"'ve\", 67: 'have'}], [{65: 'You', 67: 'you'}, {65: 've', 67: 'have'}], [{65: 'we', 67: 'we'}, {65: \"'ve\", 67: 'have'}], [{65: 'we', 67: 'we'}, {65: 've', 67: 'have'}], [{65: 'We', 67: 'we'}, {65: \"'ve\", 67: 'have'}], [{65: 'We', 67: 'we'}, {65: 've', 67: 'have'}], [{65: 'they', 67: 'they'}, {65: \"'ve\", 67: 'have'}], [{65: 'they', 67: 'they'}, {65: 've', 67: 'have'}], [{65: 'They', 67: 'they'}, {65: \"'ve\", 67: 'have'}], [{65: 'They', 67: 'they'}, {65: 've', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: \"'re\", 67: 'are'}], [{65: 'you', 67: 'you'}, {65: 're', 67: 'are'}], [{65: 'You', 67: 'you'}, {65: \"'re\", 67: 'are'}], [{65: 'You', 67: 'you'}, {65: 're', 67: 'are'}], [{65: 'we', 67: 'we'}, {65: \"'re\", 67: 'are'}], [{65: 'We', 67: 'we'}, {65: \"'re\", 67: 'are'}], [{65: 'they', 67: 'they'}, {65: \"'re\", 67: 'are'}], [{65: 'they', 67: 'they'}, {65: 're', 67: 'are'}], [{65: 'They', 67: 'they'}, {65: \"'re\", 67: 'are'}], [{65: 'They', 67: 'they'}, {65: 're', 67: 'are'}], [{65: 'he', 67: 'he'}, {65: \"'s\", 67: \"'s\"}], [{65: 'he', 67: 'he'}, {65: 's'}], [{65: 'He', 67: 'he'}, {65: \"'s\", 67: \"'s\"}], [{65: 'He', 67: 'he'}, {65: 's'}], [{65: 'she', 67: 'she'}, {65: \"'s\", 67: \"'s\"}], [{65: 'she', 67: 'she'}, {65: 's'}], [{65: 'She', 67: 'she'}, {65: \"'s\", 67: \"'s\"}], [{65: 'She', 67: 'she'}, {65: 's'}], [{65: 'it', 67: 'it'}, {65: \"'s\", 67: \"'s\"}], [{65: 'It', 67: 'it'}, {65: \"'s\", 67: \"'s\"}], [{65: 'who', 67: 'who'}, {65: \"'s\", 67: \"'s\"}], [{65: 'who', 67: 'who'}, {65: 's'}], [{65: 'who', 67: 'who'}, {65: \"'ll\", 67: 'will'}], [{65: 'who', 67: 'who'}, {65: 'll', 67: 'will'}], [{65: 'who', 67: 'who'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'who', 67: 'who'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'who', 67: 'who'}, {65: \"'re\", 67: 'are'}], [{65: 'who', 67: 'who'}, {65: \"'ve\"}], [{65: 'who'}, {65: 've', 67: 'have'}], [{65: 'who', 67: 'who'}, {65: \"'d\", 67: \"'d\"}], [{65: 'who', 67: 'who'}, {65: 'd', 67: \"'d\"}], [{65: 'who', 67: 'who'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'who', 67: 'who'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'Who', 67: 'who'}, {65: \"'s\", 67: \"'s\"}], [{65: 'Who', 67: 'who'}, {65: 's'}], [{65: 'Who', 67: 'who'}, {65: \"'ll\", 67: 'will'}], [{65: 'Who', 67: 'who'}, {65: 'll', 67: 'will'}], [{65: 'Who', 67: 'who'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'Who', 67: 'who'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'Who', 67: 'who'}, {65: \"'re\", 67: 'are'}], [{65: 'Who', 67: 'who'}, {65: \"'ve\"}], [{65: 'Who'}, {65: 've', 67: 'have'}], [{65: 'Who', 67: 'who'}, {65: \"'d\", 67: \"'d\"}], [{65: 'Who', 67: 'who'}, {65: 'd', 67: \"'d\"}], [{65: 'Who', 67: 'who'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'Who', 67: 'who'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'what', 67: 'what'}, {65: \"'s\", 67: \"'s\"}], [{65: 'what', 67: 'what'}, {65: 's'}], [{65: 'what', 67: 'what'}, {65: \"'ll\", 67: 'will'}], [{65: 'what', 67: 'what'}, {65: 'll', 67: 'will'}], [{65: 'what', 67: 'what'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'what', 67: 'what'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'what', 67: 'what'}, {65: \"'re\", 67: 'are'}], [{65: 'what', 67: 'what'}, {65: 're', 67: 'are'}], [{65: 'what', 67: 'what'}, {65: \"'ve\"}], [{65: 'what'}, {65: 've', 67: 'have'}], [{65: 'what', 67: 'what'}, {65: \"'d\", 67: \"'d\"}], [{65: 'what', 67: 'what'}, {65: 'd', 67: \"'d\"}], [{65: 'what', 67: 'what'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'what', 67: 'what'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'What', 67: 'what'}, {65: \"'s\", 67: \"'s\"}], [{65: 'What', 67: 'what'}, {65: 's'}], [{65: 'What', 67: 'what'}, {65: \"'ll\", 67: 'will'}], [{65: 'What', 67: 'what'}, {65: 'll', 67: 'will'}], [{65: 'What', 67: 'what'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'What', 67: 'what'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'What', 67: 'what'}, {65: \"'re\", 67: 'are'}], [{65: 'What', 67: 'what'}, {65: 're', 67: 'are'}], [{65: 'What', 67: 'what'}, {65: \"'ve\"}], [{65: 'What'}, {65: 've', 67: 'have'}], [{65: 'What', 67: 'what'}, {65: \"'d\", 67: \"'d\"}], [{65: 'What', 67: 'what'}, {65: 'd', 67: \"'d\"}], [{65: 'What', 67: 'what'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'What', 67: 'what'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'when', 67: 'when'}, {65: \"'s\", 67: \"'s\"}], [{65: 'when', 67: 'when'}, {65: 's'}], [{65: 'when', 67: 'when'}, {65: \"'ll\", 67: 'will'}], [{65: 'when', 67: 'when'}, {65: 'll', 67: 'will'}], [{65: 'when', 67: 'when'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'when', 67: 'when'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'when', 67: 'when'}, {65: \"'re\", 67: 'are'}], [{65: 'when', 67: 'when'}, {65: 're', 67: 'are'}], [{65: 'when', 67: 'when'}, {65: \"'ve\"}], [{65: 'when'}, {65: 've', 67: 'have'}], [{65: 'when', 67: 'when'}, {65: \"'d\", 67: \"'d\"}], [{65: 'when', 67: 'when'}, {65: 'd', 67: \"'d\"}], [{65: 'when', 67: 'when'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'when', 67: 'when'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'When', 67: 'when'}, {65: \"'s\", 67: \"'s\"}], [{65: 'When', 67: 'when'}, {65: 's'}], [{65: 'When', 67: 'when'}, {65: \"'ll\", 67: 'will'}], [{65: 'When', 67: 'when'}, {65: 'll', 67: 'will'}], [{65: 'When', 67: 'when'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'When', 67: 'when'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'When', 67: 'when'}, {65: \"'re\", 67: 'are'}], [{65: 'When', 67: 'when'}, {65: 're', 67: 'are'}], [{65: 'When', 67: 'when'}, {65: \"'ve\"}], [{65: 'When'}, {65: 've', 67: 'have'}], [{65: 'When', 67: 'when'}, {65: \"'d\", 67: \"'d\"}], [{65: 'When', 67: 'when'}, {65: 'd', 67: \"'d\"}], [{65: 'When', 67: 'when'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'When', 67: 'when'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'where', 67: 'where'}, {65: \"'s\", 67: \"'s\"}], [{65: 'where', 67: 'where'}, {65: 's'}], [{65: 'where', 67: 'where'}, {65: \"'ll\", 67: 'will'}], [{65: 'where', 67: 'where'}, {65: 'll', 67: 'will'}], [{65: 'where', 67: 'where'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'where', 67: 'where'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'where', 67: 'where'}, {65: \"'re\", 67: 'are'}], [{65: 'where', 67: 'where'}, {65: 're', 67: 'are'}], [{65: 'where', 67: 'where'}, {65: \"'ve\"}], [{65: 'where'}, {65: 've', 67: 'have'}], [{65: 'where', 67: 'where'}, {65: \"'d\", 67: \"'d\"}], [{65: 'where', 67: 'where'}, {65: 'd', 67: \"'d\"}], [{65: 'where', 67: 'where'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'where', 67: 'where'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'Where', 67: 'where'}, {65: \"'s\", 67: \"'s\"}], [{65: 'Where', 67: 'where'}, {65: 's'}], [{65: 'Where', 67: 'where'}, {65: \"'ll\", 67: 'will'}], [{65: 'Where', 67: 'where'}, {65: 'll', 67: 'will'}], [{65: 'Where', 67: 'where'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'Where', 67: 'where'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'Where', 67: 'where'}, {65: \"'re\", 67: 'are'}], [{65: 'Where', 67: 'where'}, {65: 're', 67: 'are'}], [{65: 'Where', 67: 'where'}, {65: \"'ve\"}], [{65: 'Where'}, {65: 've', 67: 'have'}], [{65: 'Where', 67: 'where'}, {65: \"'d\", 67: \"'d\"}], [{65: 'Where', 67: 'where'}, {65: 'd', 67: \"'d\"}], [{65: 'Where', 67: 'where'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'Where', 67: 'where'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'why', 67: 'why'}, {65: \"'s\", 67: \"'s\"}], [{65: 'why', 67: 'why'}, {65: 's'}], [{65: 'why', 67: 'why'}, {65: \"'ll\", 67: 'will'}], [{65: 'why', 67: 'why'}, {65: 'll', 67: 'will'}], [{65: 'why', 67: 'why'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'why', 67: 'why'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'why', 67: 'why'}, {65: \"'re\", 67: 'are'}], [{65: 'why', 67: 'why'}, {65: 're', 67: 'are'}], [{65: 'why', 67: 'why'}, {65: \"'ve\"}], [{65: 'why'}, {65: 've', 67: 'have'}], [{65: 'why', 67: 'why'}, {65: \"'d\", 67: \"'d\"}], [{65: 'why', 67: 'why'}, {65: 'd', 67: \"'d\"}], [{65: 'why', 67: 'why'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'why', 67: 'why'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'Why', 67: 'why'}, {65: \"'s\", 67: \"'s\"}], [{65: 'Why', 67: 'why'}, {65: 's'}], [{65: 'Why', 67: 'why'}, {65: \"'ll\", 67: 'will'}], [{65: 'Why', 67: 'why'}, {65: 'll', 67: 'will'}], [{65: 'Why', 67: 'why'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'Why', 67: 'why'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'Why', 67: 'why'}, {65: \"'re\", 67: 'are'}], [{65: 'Why', 67: 'why'}, {65: 're', 67: 'are'}], [{65: 'Why', 67: 'why'}, {65: \"'ve\"}], [{65: 'Why'}, {65: 've', 67: 'have'}], [{65: 'Why', 67: 'why'}, {65: \"'d\", 67: \"'d\"}], [{65: 'Why', 67: 'why'}, {65: 'd', 67: \"'d\"}], [{65: 'Why', 67: 'why'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'Why', 67: 'why'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'how', 67: 'how'}, {65: \"'s\", 67: \"'s\"}], [{65: 'how', 67: 'how'}, {65: 's'}], [{65: 'how', 67: 'how'}, {65: \"'ll\", 67: 'will'}], [{65: 'how', 67: 'how'}, {65: 'll', 67: 'will'}], [{65: 'how', 67: 'how'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'how', 67: 'how'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'how', 67: 'how'}, {65: \"'re\", 67: 'are'}], [{65: 'how', 67: 'how'}, {65: 're', 67: 'are'}], [{65: 'how', 67: 'how'}, {65: \"'ve\"}], [{65: 'how'}, {65: 've', 67: 'have'}], [{65: 'how', 67: 'how'}, {65: \"'d\", 67: \"'d\"}], [{65: 'how', 67: 'how'}, {65: 'd', 67: \"'d\"}], [{65: 'how', 67: 'how'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'how', 67: 'how'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'How', 67: 'how'}, {65: \"'s\", 67: \"'s\"}], [{65: 'How', 67: 'how'}, {65: 's'}], [{65: 'How', 67: 'how'}, {65: \"'ll\", 67: 'will'}], [{65: 'How', 67: 'how'}, {65: 'll', 67: 'will'}], [{65: 'How', 67: 'how'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'How', 67: 'how'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'How', 67: 'how'}, {65: \"'re\", 67: 'are'}], [{65: 'How', 67: 'how'}, {65: 're', 67: 'are'}], [{65: 'How', 67: 'how'}, {65: \"'ve\"}], [{65: 'How'}, {65: 've', 67: 'have'}], [{65: 'How', 67: 'how'}, {65: \"'d\", 67: \"'d\"}], [{65: 'How', 67: 'how'}, {65: 'd', 67: \"'d\"}], [{65: 'How', 67: 'how'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'How', 67: 'how'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'there', 67: 'there'}, {65: \"'s\", 67: \"'s\"}], [{65: 'there', 67: 'there'}, {65: 's'}], [{65: 'there', 67: 'there'}, {65: \"'ll\", 67: 'will'}], [{65: 'there', 67: 'there'}, {65: 'll', 67: 'will'}], [{65: 'there', 67: 'there'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'there', 67: 'there'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'there', 67: 'there'}, {65: \"'re\", 67: 'are'}], [{65: 'there', 67: 'there'}, {65: 're', 67: 'are'}], [{65: 'there', 67: 'there'}, {65: \"'ve\"}], [{65: 'there'}, {65: 've', 67: 'have'}], [{65: 'there', 67: 'there'}, {65: \"'d\", 67: \"'d\"}], [{65: 'there', 67: 'there'}, {65: 'd', 67: \"'d\"}], [{65: 'there', 67: 'there'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'there', 67: 'there'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'There', 67: 'there'}, {65: \"'s\", 67: \"'s\"}], [{65: 'There', 67: 'there'}, {65: 's'}], [{65: 'There', 67: 'there'}, {65: \"'ll\", 67: 'will'}], [{65: 'There', 67: 'there'}, {65: 'll', 67: 'will'}], [{65: 'There', 67: 'there'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'There', 67: 'there'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'There', 67: 'there'}, {65: \"'re\", 67: 'are'}], [{65: 'There', 67: 'there'}, {65: 're', 67: 'are'}], [{65: 'There', 67: 'there'}, {65: \"'ve\"}], [{65: 'There'}, {65: 've', 67: 'have'}], [{65: 'There', 67: 'there'}, {65: \"'d\", 67: \"'d\"}], [{65: 'There', 67: 'there'}, {65: 'd', 67: \"'d\"}], [{65: 'There', 67: 'there'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'There', 67: 'there'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'that', 67: 'that'}, {65: \"'s\", 67: \"'s\"}], [{65: 'that', 67: 'that'}, {65: 's'}], [{65: 'that', 67: 'that'}, {65: \"'ll\", 67: 'will'}], [{65: 'that', 67: 'that'}, {65: 'll', 67: 'will'}], [{65: 'that', 67: 'that'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'that', 67: 'that'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'that', 67: 'that'}, {65: \"'re\", 67: 'are'}], [{65: 'that', 67: 'that'}, {65: 're', 67: 'are'}], [{65: 'that', 67: 'that'}, {65: \"'ve\"}], [{65: 'that'}, {65: 've', 67: 'have'}], [{65: 'that', 67: 'that'}, {65: \"'d\", 67: \"'d\"}], [{65: 'that', 67: 'that'}, {65: 'd', 67: \"'d\"}], [{65: 'that', 67: 'that'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'that', 67: 'that'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'That', 67: 'that'}, {65: \"'s\", 67: \"'s\"}], [{65: 'That', 67: 'that'}, {65: 's'}], [{65: 'That', 67: 'that'}, {65: \"'ll\", 67: 'will'}], [{65: 'That', 67: 'that'}, {65: 'll', 67: 'will'}], [{65: 'That', 67: 'that'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'That', 67: 'that'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'That', 67: 'that'}, {65: \"'re\", 67: 'are'}], [{65: 'That', 67: 'that'}, {65: 're', 67: 'are'}], [{65: 'That', 67: 'that'}, {65: \"'ve\"}], [{65: 'That'}, {65: 've', 67: 'have'}], [{65: 'That', 67: 'that'}, {65: \"'d\", 67: \"'d\"}], [{65: 'That', 67: 'that'}, {65: 'd', 67: \"'d\"}], [{65: 'That', 67: 'that'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'That', 67: 'that'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'this', 67: 'this'}, {65: \"'s\", 67: \"'s\"}], [{65: 'this', 67: 'this'}, {65: 's'}], [{65: 'this', 67: 'this'}, {65: \"'ll\", 67: 'will'}], [{65: 'this', 67: 'this'}, {65: 'll', 67: 'will'}], [{65: 'this', 67: 'this'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'this', 67: 'this'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'this', 67: 'this'}, {65: \"'re\", 67: 'are'}], [{65: 'this', 67: 'this'}, {65: 're', 67: 'are'}], [{65: 'this', 67: 'this'}, {65: \"'ve\"}], [{65: 'this'}, {65: 've', 67: 'have'}], [{65: 'this', 67: 'this'}, {65: \"'d\", 67: \"'d\"}], [{65: 'this', 67: 'this'}, {65: 'd', 67: \"'d\"}], [{65: 'this', 67: 'this'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'this', 67: 'this'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'This', 67: 'this'}, {65: \"'s\", 67: \"'s\"}], [{65: 'This', 67: 'this'}, {65: 's'}], [{65: 'This', 67: 'this'}, {65: \"'ll\", 67: 'will'}], [{65: 'This', 67: 'this'}, {65: 'll', 67: 'will'}], [{65: 'This', 67: 'this'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'This', 67: 'this'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'This', 67: 'this'}, {65: \"'re\", 67: 'are'}], [{65: 'This', 67: 'this'}, {65: 're', 67: 'are'}], [{65: 'This', 67: 'this'}, {65: \"'ve\"}], [{65: 'This'}, {65: 've', 67: 'have'}], [{65: 'This', 67: 'this'}, {65: \"'d\", 67: \"'d\"}], [{65: 'This', 67: 'this'}, {65: 'd', 67: \"'d\"}], [{65: 'This', 67: 'this'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'This', 67: 'this'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'these', 67: 'these'}, {65: \"'s\", 67: \"'s\"}], [{65: 'these', 67: 'these'}, {65: 's'}], [{65: 'these', 67: 'these'}, {65: \"'ll\", 67: 'will'}], [{65: 'these', 67: 'these'}, {65: 'll', 67: 'will'}], [{65: 'these', 67: 'these'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'these', 67: 'these'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'these', 67: 'these'}, {65: \"'re\", 67: 'are'}], [{65: 'these', 67: 'these'}, {65: 're', 67: 'are'}], [{65: 'these', 67: 'these'}, {65: \"'ve\"}], [{65: 'these'}, {65: 've', 67: 'have'}], [{65: 'these', 67: 'these'}, {65: \"'d\", 67: \"'d\"}], [{65: 'these', 67: 'these'}, {65: 'd', 67: \"'d\"}], [{65: 'these', 67: 'these'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'these', 67: 'these'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'These', 67: 'these'}, {65: \"'s\", 67: \"'s\"}], [{65: 'These', 67: 'these'}, {65: 's'}], [{65: 'These', 67: 'these'}, {65: \"'ll\", 67: 'will'}], [{65: 'These', 67: 'these'}, {65: 'll', 67: 'will'}], [{65: 'These', 67: 'these'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'These', 67: 'these'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'These', 67: 'these'}, {65: \"'re\", 67: 'are'}], [{65: 'These', 67: 'these'}, {65: 're', 67: 'are'}], [{65: 'These', 67: 'these'}, {65: \"'ve\"}], [{65: 'These'}, {65: 've', 67: 'have'}], [{65: 'These', 67: 'these'}, {65: \"'d\", 67: \"'d\"}], [{65: 'These', 67: 'these'}, {65: 'd', 67: \"'d\"}], [{65: 'These', 67: 'these'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'These', 67: 'these'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'those', 67: 'those'}, {65: \"'s\", 67: \"'s\"}], [{65: 'those', 67: 'those'}, {65: 's'}], [{65: 'those', 67: 'those'}, {65: \"'ll\", 67: 'will'}], [{65: 'those', 67: 'those'}, {65: 'll', 67: 'will'}], [{65: 'those', 67: 'those'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'those', 67: 'those'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'those', 67: 'those'}, {65: \"'re\", 67: 'are'}], [{65: 'those', 67: 'those'}, {65: 're', 67: 'are'}], [{65: 'those', 67: 'those'}, {65: \"'ve\"}], [{65: 'those'}, {65: 've', 67: 'have'}], [{65: 'those', 67: 'those'}, {65: \"'d\", 67: \"'d\"}], [{65: 'those', 67: 'those'}, {65: 'd', 67: \"'d\"}], [{65: 'those', 67: 'those'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'those', 67: 'those'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'Those', 67: 'those'}, {65: \"'s\", 67: \"'s\"}], [{65: 'Those', 67: 'those'}, {65: 's'}], [{65: 'Those', 67: 'those'}, {65: \"'ll\", 67: 'will'}], [{65: 'Those', 67: 'those'}, {65: 'll', 67: 'will'}], [{65: 'Those', 67: 'those'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'Those', 67: 'those'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'Those', 67: 'those'}, {65: \"'re\", 67: 'are'}], [{65: 'Those', 67: 'those'}, {65: 're', 67: 'are'}], [{65: 'Those', 67: 'those'}, {65: \"'ve\"}], [{65: 'Those'}, {65: 've', 67: 'have'}], [{65: 'Those', 67: 'those'}, {65: \"'d\", 67: \"'d\"}], [{65: 'Those', 67: 'those'}, {65: 'd', 67: \"'d\"}], [{65: 'Those', 67: 'those'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'Those', 67: 'those'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'ca', 67: 'can'}, {65: \"n't\", 67: 'not'}], [{65: 'ca', 67: 'can'}, {65: 'nt', 67: 'not'}], [{65: 'ca', 67: 'can'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'ca', 67: 'can'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Ca', 67: 'can'}, {65: \"n't\", 67: 'not'}], [{65: 'Ca', 67: 'can'}, {65: 'nt', 67: 'not'}], [{65: 'Ca', 67: 'can'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Ca', 67: 'can'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'could', 67: 'could'}, {65: \"n't\", 67: 'not'}], [{65: 'could', 67: 'could'}, {65: 'nt', 67: 'not'}], [{65: 'could', 67: 'could'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'could', 67: 'could'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Could', 67: 'could'}, {65: \"n't\", 67: 'not'}], [{65: 'Could', 67: 'could'}, {65: 'nt', 67: 'not'}], [{65: 'Could', 67: 'could'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Could', 67: 'could'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'do', 67: 'do'}, {65: \"n't\", 67: 'not'}], [{65: 'do', 67: 'do'}, {65: 'nt', 67: 'not'}], [{65: 'do', 67: 'do'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'do', 67: 'do'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Do', 67: 'do'}, {65: \"n't\", 67: 'not'}], [{65: 'Do', 67: 'do'}, {65: 'nt', 67: 'not'}], [{65: 'Do', 67: 'do'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Do', 67: 'do'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'does', 67: 'does'}, {65: \"n't\", 67: 'not'}], [{65: 'does', 67: 'does'}, {65: 'nt', 67: 'not'}], [{65: 'does', 67: 'does'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'does', 67: 'does'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Does', 67: 'does'}, {65: \"n't\", 67: 'not'}], [{65: 'Does', 67: 'does'}, {65: 'nt', 67: 'not'}], [{65: 'Does', 67: 'does'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Does', 67: 'does'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'did', 67: 'do'}, {65: \"n't\", 67: 'not'}], [{65: 'did', 67: 'do'}, {65: 'nt', 67: 'not'}], [{65: 'did', 67: 'do'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'did', 67: 'do'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Did', 67: 'do'}, {65: \"n't\", 67: 'not'}], [{65: 'Did', 67: 'do'}, {65: 'nt', 67: 'not'}], [{65: 'Did', 67: 'do'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Did', 67: 'do'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'had', 67: 'have'}, {65: \"n't\", 67: 'not'}], [{65: 'had', 67: 'have'}, {65: 'nt', 67: 'not'}], [{65: 'had', 67: 'have'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'had', 67: 'have'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Had', 67: 'have'}, {65: \"n't\", 67: 'not'}], [{65: 'Had', 67: 'have'}, {65: 'nt', 67: 'not'}], [{65: 'Had', 67: 'have'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Had', 67: 'have'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'may', 67: 'may'}, {65: \"n't\", 67: 'not'}], [{65: 'may', 67: 'may'}, {65: 'nt', 67: 'not'}], [{65: 'may', 67: 'may'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'may', 67: 'may'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'May', 67: 'may'}, {65: \"n't\", 67: 'not'}], [{65: 'May', 67: 'may'}, {65: 'nt', 67: 'not'}], [{65: 'May', 67: 'may'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'May', 67: 'may'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'might', 67: 'might'}, {65: \"n't\", 67: 'not'}], [{65: 'might', 67: 'might'}, {65: 'nt', 67: 'not'}], [{65: 'might', 67: 'might'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'might', 67: 'might'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Might', 67: 'might'}, {65: \"n't\", 67: 'not'}], [{65: 'Might', 67: 'might'}, {65: 'nt', 67: 'not'}], [{65: 'Might', 67: 'might'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Might', 67: 'might'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'must', 67: 'must'}, {65: \"n't\", 67: 'not'}], [{65: 'must', 67: 'must'}, {65: 'nt', 67: 'not'}], [{65: 'must', 67: 'must'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'must', 67: 'must'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Must', 67: 'must'}, {65: \"n't\", 67: 'not'}], [{65: 'Must', 67: 'must'}, {65: 'nt', 67: 'not'}], [{65: 'Must', 67: 'must'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Must', 67: 'must'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'need', 67: 'need'}, {65: \"n't\", 67: 'not'}], [{65: 'need', 67: 'need'}, {65: 'nt', 67: 'not'}], [{65: 'need', 67: 'need'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'need', 67: 'need'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Need', 67: 'need'}, {65: \"n't\", 67: 'not'}], [{65: 'Need', 67: 'need'}, {65: 'nt', 67: 'not'}], [{65: 'Need', 67: 'need'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Need', 67: 'need'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'ought', 67: 'ought'}, {65: \"n't\", 67: 'not'}], [{65: 'ought', 67: 'ought'}, {65: 'nt', 67: 'not'}], [{65: 'ought', 67: 'ought'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'ought', 67: 'ought'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Ought', 67: 'ought'}, {65: \"n't\", 67: 'not'}], [{65: 'Ought', 67: 'ought'}, {65: 'nt', 67: 'not'}], [{65: 'Ought', 67: 'ought'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Ought', 67: 'ought'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'sha', 67: 'shall'}, {65: \"n't\", 67: 'not'}], [{65: 'sha', 67: 'shall'}, {65: 'nt', 67: 'not'}], [{65: 'sha', 67: 'shall'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'sha', 67: 'shall'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Sha', 67: 'shall'}, {65: \"n't\", 67: 'not'}], [{65: 'Sha', 67: 'shall'}, {65: 'nt', 67: 'not'}], [{65: 'Sha', 67: 'shall'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Sha', 67: 'shall'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'should', 67: 'should'}, {65: \"n't\", 67: 'not'}], [{65: 'should', 67: 'should'}, {65: 'nt', 67: 'not'}], [{65: 'should', 67: 'should'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'should', 67: 'should'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Should', 67: 'should'}, {65: \"n't\", 67: 'not'}], [{65: 'Should', 67: 'should'}, {65: 'nt', 67: 'not'}], [{65: 'Should', 67: 'should'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Should', 67: 'should'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'wo', 67: 'will'}, {65: \"n't\", 67: 'not'}], [{65: 'wo', 67: 'will'}, {65: 'nt', 67: 'not'}], [{65: 'wo', 67: 'will'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'wo', 67: 'will'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Wo', 67: 'will'}, {65: \"n't\", 67: 'not'}], [{65: 'Wo', 67: 'will'}, {65: 'nt', 67: 'not'}], [{65: 'Wo', 67: 'will'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Wo', 67: 'will'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'would', 67: 'would'}, {65: \"n't\", 67: 'not'}], [{65: 'would', 67: 'would'}, {65: 'nt', 67: 'not'}], [{65: 'would', 67: 'would'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'would', 67: 'would'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Would', 67: 'would'}, {65: \"n't\", 67: 'not'}], [{65: 'Would', 67: 'would'}, {65: 'nt', 67: 'not'}], [{65: 'Would', 67: 'would'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Would', 67: 'would'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'could', 67: 'could'}, {65: \"'ve\"}], [{65: 'could', 67: 'could'}, {65: 've'}], [{65: 'Could', 67: 'could'}, {65: \"'ve\"}], [{65: 'Could', 67: 'could'}, {65: 've'}], [{65: 'might', 67: 'might'}, {65: \"'ve\"}], [{65: 'might', 67: 'might'}, {65: 've'}], [{65: 'Might', 67: 'might'}, {65: \"'ve\"}], [{65: 'Might', 67: 'might'}, {65: 've'}], [{65: 'must', 67: 'must'}, {65: \"'ve\"}], [{65: 'must', 67: 'must'}, {65: 've'}], [{65: 'Must', 67: 'must'}, {65: \"'ve\"}], [{65: 'Must', 67: 'must'}, {65: 've'}], [{65: 'should', 67: 'should'}, {65: \"'ve\"}], [{65: 'should', 67: 'should'}, {65: 've'}], [{65: 'Should', 67: 'should'}, {65: \"'ve\"}], [{65: 'Should', 67: 'should'}, {65: 've'}], [{65: 'would', 67: 'would'}, {65: \"'ve\"}], [{65: 'would', 67: 'would'}, {65: 've'}], [{65: 'Would', 67: 'would'}, {65: \"'ve\"}], [{65: 'Would', 67: 'would'}, {65: 've'}], [{65: 'ai'}, {65: \"n't\", 67: 'not'}], [{65: 'ai'}, {65: 'nt', 67: 'not'}], [{65: 'Ai'}, {65: \"n't\", 67: 'not'}], [{65: 'Ai'}, {65: 'nt', 67: 'not'}], [{65: 'are', 67: 'are'}, {65: \"n't\", 67: 'not'}], [{65: 'are', 67: 'are'}, {65: 'nt', 67: 'not'}], [{65: 'Are', 67: 'are'}, {65: \"n't\", 67: 'not'}], [{65: 'Are', 67: 'are'}, {65: 'nt', 67: 'not'}], [{65: 'is', 67: 'is'}, {65: \"n't\", 67: 'not'}], [{65: 'is', 67: 'is'}, {65: 'nt', 67: 'not'}], [{65: 'Is', 67: 'is'}, {65: \"n't\", 67: 'not'}], [{65: 'Is', 67: 'is'}, {65: 'nt', 67: 'not'}], [{65: 'was', 67: 'was'}, {65: \"n't\", 67: 'not'}], [{65: 'was', 67: 'was'}, {65: 'nt', 67: 'not'}], [{65: 'Was', 67: 'was'}, {65: \"n't\", 67: 'not'}], [{65: 'Was', 67: 'was'}, {65: 'nt', 67: 'not'}], [{65: 'were', 67: 'were'}, {65: \"n't\", 67: 'not'}], [{65: 'were', 67: 'were'}, {65: 'nt', 67: 'not'}], [{65: 'Were', 67: 'were'}, {65: \"n't\", 67: 'not'}], [{65: 'Were', 67: 'were'}, {65: 'nt', 67: 'not'}], [{65: 'have', 67: 'have'}, {65: \"n't\", 67: 'not'}], [{65: 'have', 67: 'have'}, {65: 'nt', 67: 'not'}], [{65: 'Have', 67: 'have'}, {65: \"n't\", 67: 'not'}], [{65: 'Have', 67: 'have'}, {65: 'nt', 67: 'not'}], [{65: 'has', 67: 'has'}, {65: \"n't\", 67: 'not'}], [{65: 'has', 67: 'has'}, {65: 'nt', 67: 'not'}], [{65: 'Has', 67: 'has'}, {65: \"n't\", 67: 'not'}], [{65: 'Has', 67: 'has'}, {65: 'nt', 67: 'not'}], [{65: 'dare', 67: 'dare'}, {65: \"n't\", 67: 'not'}], [{65: 'dare', 67: 'dare'}, {65: 'nt', 67: 'not'}], [{65: 'Dare', 67: 'dare'}, {65: \"n't\", 67: 'not'}], [{65: 'Dare', 67: 'dare'}, {65: 'nt', 67: 'not'}], [{65: 'doin', 67: 'doing'}], [{65: \"doin'\", 67: 'doing'}], [{65: 'Doin', 67: 'doing'}], [{65: \"Doin'\", 67: 'doing'}], [{65: 'goin', 67: 'going'}], [{65: \"goin'\", 67: 'going'}], [{65: 'Goin', 67: 'going'}], [{65: \"Goin'\", 67: 'going'}], [{65: 'nothin', 67: 'nothing'}], [{65: \"nothin'\", 67: 'nothing'}], [{65: 'Nothin', 67: 'nothing'}], [{65: \"Nothin'\", 67: 'nothing'}], [{65: 'nuthin', 67: 'nothing'}], [{65: \"nuthin'\", 67: 'nothing'}], [{65: 'Nuthin', 67: 'nothing'}], [{65: \"Nuthin'\", 67: 'nothing'}], [{65: 'ol', 67: 'old'}], [{65: \"ol'\", 67: 'old'}], [{65: 'Ol', 67: 'old'}], [{65: \"Ol'\", 67: 'old'}], [{65: 'somethin', 67: 'something'}], [{65: \"somethin'\", 67: 'something'}], [{65: 'Somethin', 67: 'something'}], [{65: \"Somethin'\", 67: 'something'}], [{65: 'em', 67: 'them'}], [{65: \"'em\", 67: 'them'}], [{65: 'll', 67: 'will'}], [{65: \"'ll\", 67: 'will'}], [{65: 'nuff', 67: 'enough'}], [{65: \"'nuff\", 67: 'enough'}], [{65: '1'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '1'}, {65: 'am', 67: 'a.m.'}], [{65: '1'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '1'}, {65: 'pm', 67: 'p.m.'}], [{65: '2'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '2'}, {65: 'am', 67: 'a.m.'}], [{65: '2'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '2'}, {65: 'pm', 67: 'p.m.'}], [{65: '3'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '3'}, {65: 'am', 67: 'a.m.'}], [{65: '3'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '3'}, {65: 'pm', 67: 'p.m.'}], [{65: '4'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '4'}, {65: 'am', 67: 'a.m.'}], [{65: '4'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '4'}, {65: 'pm', 67: 'p.m.'}], [{65: '5'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '5'}, {65: 'am', 67: 'a.m.'}], [{65: '5'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '5'}, {65: 'pm', 67: 'p.m.'}], [{65: '6'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '6'}, {65: 'am', 67: 'a.m.'}], [{65: '6'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '6'}, {65: 'pm', 67: 'p.m.'}], [{65: '7'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '7'}, {65: 'am', 67: 'a.m.'}], [{65: '7'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '7'}, {65: 'pm', 67: 'p.m.'}], [{65: '8'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '8'}, {65: 'am', 67: 'a.m.'}], [{65: '8'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '8'}, {65: 'pm', 67: 'p.m.'}], [{65: '9'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '9'}, {65: 'am', 67: 'a.m.'}], [{65: '9'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '9'}, {65: 'pm', 67: 'p.m.'}], [{65: '10'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '10'}, {65: 'am', 67: 'a.m.'}], [{65: '10'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '10'}, {65: 'pm', 67: 'p.m.'}], [{65: '11'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '11'}, {65: 'am', 67: 'a.m.'}], [{65: '11'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '11'}, {65: 'pm', 67: 'p.m.'}], [{65: '12'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '12'}, {65: 'am', 67: 'a.m.'}], [{65: '12'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '12'}, {65: 'pm', 67: 'p.m.'}], [{65: \"y'\", 67: 'you'}, {65: 'all'}], [{65: 'y', 67: 'you'}, {65: 'all'}], [{65: 'how'}, {65: \"'d\"}, {65: \"'y\", 67: 'you'}], [{65: 'How', 67: 'how'}, {65: \"'d\"}, {65: \"'y\", 67: 'you'}], [{65: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'not'}, {65: 've', 67: 'have'}], [{65: 'Not', 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Not', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'can'}, {65: 'not'}], [{65: 'Can', 67: 'can'}, {65: 'not'}], [{65: 'gon', 67: 'going'}, {65: 'na', 67: 'to'}], [{65: 'Gon', 67: 'going'}, {65: 'na', 67: 'to'}], [{65: 'got'}, {65: 'ta', 67: 'to'}], [{65: 'Got', 67: 'got'}, {65: 'ta', 67: 'to'}], [{65: 'let'}, {65: \"'s\", 67: 'us'}], [{65: 'Let', 67: 'let'}, {65: \"'s\", 67: 'us'}], [{65: \"c'm\", 67: 'come'}, {65: 'on'}], [{65: \"C'm\", 67: 'come'}, {65: 'on'}], [{65: \"'S\", 67: \"'s\"}], [{65: \"'s\", 67: \"'s\"}], [{65: '‘S', 67: \"'s\"}], [{65: '‘s', 67: \"'s\"}], [{65: 'and/or', 67: 'and/or'}], [{65: 'w/o', 67: 'without'}], [{65: \"'re\", 67: 'are'}], [{65: \"'Cause\", 67: 'because'}], [{65: \"'cause\", 67: 'because'}], [{65: \"'cos\", 67: 'because'}], [{65: \"'Cos\", 67: 'because'}], [{65: \"'coz\", 67: 'because'}], [{65: \"'Coz\", 67: 'because'}], [{65: \"'cuz\", 67: 'because'}], [{65: \"'Cuz\", 67: 'because'}], [{65: \"'bout\", 67: 'about'}], [{65: \"ma'am\", 67: 'madam'}], [{65: \"Ma'am\", 67: 'madam'}], [{65: \"o'clock\", 67: \"o'clock\"}], [{65: \"O'clock\", 67: \"o'clock\"}], [{65: \"lovin'\", 67: 'loving'}], [{65: \"Lovin'\", 67: 'loving'}], [{65: 'lovin', 67: 'loving'}], [{65: 'Lovin', 67: 'loving'}], [{65: \"havin'\", 67: 'having'}], [{65: \"Havin'\", 67: 'having'}], [{65: 'havin', 67: 'having'}], [{65: 'Havin', 67: 'having'}], [{65: 'Mt.', 67: 'Mount'}], [{65: 'Ak.', 67: 'Alaska'}], [{65: 'Ala.', 67: 'Alabama'}], [{65: 'Apr.', 67: 'April'}], [{65: 'Ariz.', 67: 'Arizona'}], [{65: 'Ark.', 67: 'Arkansas'}], [{65: 'Aug.', 67: 'August'}], [{65: 'Calif.', 67: 'California'}], [{65: 'Colo.', 67: 'Colorado'}], [{65: 'Conn.', 67: 'Connecticut'}], [{65: 'Dec.', 67: 'December'}], [{65: 'Del.', 67: 'Delaware'}], [{65: 'Feb.', 67: 'February'}], [{65: 'Fla.', 67: 'Florida'}], [{65: 'Ga.', 67: 'Georgia'}], [{65: 'Ia.', 67: 'Iowa'}], [{65: 'Id.', 67: 'Idaho'}], [{65: 'Ill.', 67: 'Illinois'}], [{65: 'Ind.', 67: 'Indiana'}], [{65: 'Jan.', 67: 'January'}], [{65: 'Jul.', 67: 'July'}], [{65: 'Jun.', 67: 'June'}], [{65: 'Kan.', 67: 'Kansas'}], [{65: 'Kans.', 67: 'Kansas'}], [{65: 'Ky.', 67: 'Kentucky'}], [{65: 'La.', 67: 'Louisiana'}], [{65: 'Mar.', 67: 'March'}], [{65: 'Mass.', 67: 'Massachusetts'}], [{65: 'May.', 67: 'May'}], [{65: 'Mich.', 67: 'Michigan'}], [{65: 'Minn.', 67: 'Minnesota'}], [{65: 'Miss.', 67: 'Mississippi'}], [{65: 'N.C.', 67: 'North Carolina'}], [{65: 'N.D.', 67: 'North Dakota'}], [{65: 'N.H.', 67: 'New Hampshire'}], [{65: 'N.J.', 67: 'New Jersey'}], [{65: 'N.M.', 67: 'New Mexico'}], [{65: 'N.Y.', 67: 'New York'}], [{65: 'Neb.', 67: 'Nebraska'}], [{65: 'Nebr.', 67: 'Nebraska'}], [{65: 'Nev.', 67: 'Nevada'}], [{65: 'Nov.', 67: 'November'}], [{65: 'Oct.', 67: 'October'}], [{65: 'Okla.', 67: 'Oklahoma'}], [{65: 'Ore.', 67: 'Oregon'}], [{65: 'Pa.', 67: 'Pennsylvania'}], [{65: 'S.C.', 67: 'South Carolina'}], [{65: 'Sep.', 67: 'September'}], [{65: 'Sept.', 67: 'September'}], [{65: 'Tenn.', 67: 'Tennessee'}], [{65: 'Va.', 67: 'Virginia'}], [{65: 'Wash.', 67: 'Washington'}], [{65: 'Wis.', 67: 'Wisconsin'}], [{65: \"'d\"}], [{65: 'a.m.'}], [{65: 'Adm.'}], [{65: 'Bros.'}], [{65: 'co.'}], [{65: 'Co.'}], [{65: 'Corp.'}], [{65: 'D.C.'}], [{65: 'Dr.'}], [{65: 'e.g.'}], [{65: 'E.g.'}], [{65: 'E.G.'}], [{65: 'Gen.'}], [{65: 'Gov.'}], [{65: 'i.e.'}], [{65: 'I.e.'}], [{65: 'I.E.'}], [{65: 'Inc.'}], [{65: 'Jr.'}], [{65: 'Ltd.'}], [{65: 'Md.'}], [{65: 'Messrs.'}], [{65: 'Mo.'}], [{65: 'Mont.'}], [{65: 'Mr.'}], [{65: 'Mrs.'}], [{65: 'Ms.'}], [{65: 'p.m.'}], [{65: 'Ph.D.'}], [{65: 'Prof.'}], [{65: 'Rep.'}], [{65: 'Rev.'}], [{65: 'Sen.'}], [{65: 'St.'}], [{65: 'vs.'}], [{65: 'v.s.'}], [{65: '’'}], [{65: '’’'}], [{65: ':’('}], [{65: ':’-('}], [{65: ':’)'}], [{65: ':’-)'}], [{65: 'i', 67: 'i'}, {65: '’m', 67: 'am'}], [{65: 'i', 67: 'i'}, {65: '’m', 67: 'am'}, {65: 'a', 67: 'gonna'}], [{65: 'I', 67: 'i'}, {65: '’m', 67: 'am'}], [{65: 'I', 67: 'i'}, {65: '’m', 67: 'am'}, {65: 'a', 67: 'gonna'}], [{65: 'i', 67: 'i'}, {65: '’ll', 67: 'will'}], [{65: 'i', 67: 'i'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'i', 67: 'i'}, {65: '’d', 67: \"'d\"}], [{65: 'i', 67: 'i'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'I', 67: 'i'}, {65: '’ll', 67: 'will'}], [{65: 'I', 67: 'i'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'I', 67: 'i'}, {65: '’d', 67: \"'d\"}], [{65: 'I', 67: 'i'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: '’ll', 67: 'will'}], [{65: 'you', 67: 'you'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: '’d', 67: \"'d\"}], [{65: 'you', 67: 'you'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'You', 67: 'you'}, {65: '’ll', 67: 'will'}], [{65: 'You', 67: 'you'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'You', 67: 'you'}, {65: '’d', 67: \"'d\"}], [{65: 'You', 67: 'you'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'he', 67: 'he'}, {65: '’ll', 67: 'will'}], [{65: 'he', 67: 'he'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'he', 67: 'he'}, {65: '’d', 67: \"'d\"}], [{65: 'he', 67: 'he'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'He', 67: 'he'}, {65: '’ll', 67: 'will'}], [{65: 'He', 67: 'he'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'He', 67: 'he'}, {65: '’d', 67: \"'d\"}], [{65: 'He', 67: 'he'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'she', 67: 'she'}, {65: '’ll', 67: 'will'}], [{65: 'she', 67: 'she'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'she', 67: 'she'}, {65: '’d', 67: \"'d\"}], [{65: 'she', 67: 'she'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'She', 67: 'she'}, {65: '’ll', 67: 'will'}], [{65: 'She', 67: 'she'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'She', 67: 'she'}, {65: '’d', 67: \"'d\"}], [{65: 'She', 67: 'she'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'it', 67: 'it'}, {65: '’ll', 67: 'will'}], [{65: 'it', 67: 'it'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'it', 67: 'it'}, {65: '’d', 67: \"'d\"}], [{65: 'it', 67: 'it'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'It', 67: 'it'}, {65: '’ll', 67: 'will'}], [{65: 'It', 67: 'it'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'It', 67: 'it'}, {65: '’d', 67: \"'d\"}], [{65: 'It', 67: 'it'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'we', 67: 'we'}, {65: '’ll', 67: 'will'}], [{65: 'we', 67: 'we'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'we', 67: 'we'}, {65: '’d', 67: \"'d\"}], [{65: 'we', 67: 'we'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'We', 67: 'we'}, {65: '’ll', 67: 'will'}], [{65: 'We', 67: 'we'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'We', 67: 'we'}, {65: '’d', 67: \"'d\"}], [{65: 'We', 67: 'we'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'they', 67: 'they'}, {65: '’ll', 67: 'will'}], [{65: 'they', 67: 'they'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'they', 67: 'they'}, {65: '’d', 67: \"'d\"}], [{65: 'they', 67: 'they'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'They', 67: 'they'}, {65: '’ll', 67: 'will'}], [{65: 'They', 67: 'they'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'They', 67: 'they'}, {65: '’d', 67: \"'d\"}], [{65: 'They', 67: 'they'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'i', 67: 'i'}, {65: '’ve', 67: 'have'}], [{65: 'I', 67: 'i'}, {65: '’ve', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: '’ve', 67: 'have'}], [{65: 'You', 67: 'you'}, {65: '’ve', 67: 'have'}], [{65: 'we', 67: 'we'}, {65: '’ve', 67: 'have'}], [{65: 'We', 67: 'we'}, {65: '’ve', 67: 'have'}], [{65: 'they', 67: 'they'}, {65: '’ve', 67: 'have'}], [{65: 'They', 67: 'they'}, {65: '’ve', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: '’re', 67: 'are'}], [{65: 'You', 67: 'you'}, {65: '’re', 67: 'are'}], [{65: 'we', 67: 'we'}, {65: '’re', 67: 'are'}], [{65: 'We', 67: 'we'}, {65: '’re', 67: 'are'}], [{65: 'they', 67: 'they'}, {65: '’re', 67: 'are'}], [{65: 'They', 67: 'they'}, {65: '’re', 67: 'are'}], [{65: 'he', 67: 'he'}, {65: '’s', 67: \"'s\"}], [{65: 'He', 67: 'he'}, {65: '’s', 67: \"'s\"}], [{65: 'she', 67: 'she'}, {65: '’s', 67: \"'s\"}], [{65: 'She', 67: 'she'}, {65: '’s', 67: \"'s\"}], [{65: 'it', 67: 'it'}, {65: '’s', 67: \"'s\"}], [{65: 'It', 67: 'it'}, {65: '’s', 67: \"'s\"}], [{65: 'who', 67: 'who'}, {65: '’s', 67: \"'s\"}], [{65: 'who', 67: 'who'}, {65: '’ll', 67: 'will'}], [{65: 'who', 67: 'who'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'who', 67: 'who'}, {65: '’re', 67: 'are'}], [{65: 'who', 67: 'who'}, {65: '’ve'}], [{65: 'who', 67: 'who'}, {65: '’d', 67: \"'d\"}], [{65: 'who', 67: 'who'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'Who', 67: 'who'}, {65: '’s', 67: \"'s\"}], [{65: 'Who', 67: 'who'}, {65: '’ll', 67: 'will'}], [{65: 'Who', 67: 'who'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'Who', 67: 'who'}, {65: '’re', 67: 'are'}], [{65: 'Who', 67: 'who'}, {65: '’ve'}], [{65: 'Who', 67: 'who'}, {65: '’d', 67: \"'d\"}], [{65: 'Who', 67: 'who'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'what', 67: 'what'}, {65: '’s', 67: \"'s\"}], [{65: 'what', 67: 'what'}, {65: '’ll', 67: 'will'}], [{65: 'what', 67: 'what'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'what', 67: 'what'}, {65: '’re', 67: 'are'}], [{65: 'what', 67: 'what'}, {65: '’ve'}], [{65: 'what', 67: 'what'}, {65: '’d', 67: \"'d\"}], [{65: 'what', 67: 'what'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'What', 67: 'what'}, {65: '’s', 67: \"'s\"}], [{65: 'What', 67: 'what'}, {65: '’ll', 67: 'will'}], [{65: 'What', 67: 'what'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'What', 67: 'what'}, {65: '’re', 67: 'are'}], [{65: 'What', 67: 'what'}, {65: '’ve'}], [{65: 'What', 67: 'what'}, {65: '’d', 67: \"'d\"}], [{65: 'What', 67: 'what'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'when', 67: 'when'}, {65: '’s', 67: \"'s\"}], [{65: 'when', 67: 'when'}, {65: '’ll', 67: 'will'}], [{65: 'when', 67: 'when'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'when', 67: 'when'}, {65: '’re', 67: 'are'}], [{65: 'when', 67: 'when'}, {65: '’ve'}], [{65: 'when', 67: 'when'}, {65: '’d', 67: \"'d\"}], [{65: 'when', 67: 'when'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'When', 67: 'when'}, {65: '’s', 67: \"'s\"}], [{65: 'When', 67: 'when'}, {65: '’ll', 67: 'will'}], [{65: 'When', 67: 'when'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'When', 67: 'when'}, {65: '’re', 67: 'are'}], [{65: 'When', 67: 'when'}, {65: '’ve'}], [{65: 'When', 67: 'when'}, {65: '’d', 67: \"'d\"}], [{65: 'When', 67: 'when'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'where', 67: 'where'}, {65: '’s', 67: \"'s\"}], [{65: 'where', 67: 'where'}, {65: '’ll', 67: 'will'}], [{65: 'where', 67: 'where'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'where', 67: 'where'}, {65: '’re', 67: 'are'}], [{65: 'where', 67: 'where'}, {65: '’ve'}], [{65: 'where', 67: 'where'}, {65: '’d', 67: \"'d\"}], [{65: 'where', 67: 'where'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'Where', 67: 'where'}, {65: '’s', 67: \"'s\"}], [{65: 'Where', 67: 'where'}, {65: '’ll', 67: 'will'}], [{65: 'Where', 67: 'where'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'Where', 67: 'where'}, {65: '’re', 67: 'are'}], [{65: 'Where', 67: 'where'}, {65: '’ve'}], [{65: 'Where', 67: 'where'}, {65: '’d', 67: \"'d\"}], [{65: 'Where', 67: 'where'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'why', 67: 'why'}, {65: '’s', 67: \"'s\"}], [{65: 'why', 67: 'why'}, {65: '’ll', 67: 'will'}], [{65: 'why', 67: 'why'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'why', 67: 'why'}, {65: '’re', 67: 'are'}], [{65: 'why', 67: 'why'}, {65: '’ve'}], [{65: 'why', 67: 'why'}, {65: '’d', 67: \"'d\"}], [{65: 'why', 67: 'why'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'Why', 67: 'why'}, {65: '’s', 67: \"'s\"}], [{65: 'Why', 67: 'why'}, {65: '’ll', 67: 'will'}], [{65: 'Why', 67: 'why'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'Why', 67: 'why'}, {65: '’re', 67: 'are'}], [{65: 'Why', 67: 'why'}, {65: '’ve'}], [{65: 'Why', 67: 'why'}, {65: '’d', 67: \"'d\"}], [{65: 'Why', 67: 'why'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'how', 67: 'how'}, {65: '’s', 67: \"'s\"}], [{65: 'how', 67: 'how'}, {65: '’ll', 67: 'will'}], [{65: 'how', 67: 'how'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'how', 67: 'how'}, {65: '’re', 67: 'are'}], [{65: 'how', 67: 'how'}, {65: '’ve'}], [{65: 'how', 67: 'how'}, {65: '’d', 67: \"'d\"}], [{65: 'how', 67: 'how'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'How', 67: 'how'}, {65: '’s', 67: \"'s\"}], [{65: 'How', 67: 'how'}, {65: '’ll', 67: 'will'}], [{65: 'How', 67: 'how'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'How', 67: 'how'}, {65: '’re', 67: 'are'}], [{65: 'How', 67: 'how'}, {65: '’ve'}], [{65: 'How', 67: 'how'}, {65: '’d', 67: \"'d\"}], [{65: 'How', 67: 'how'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'there', 67: 'there'}, {65: '’s', 67: \"'s\"}], [{65: 'there', 67: 'there'}, {65: '’ll', 67: 'will'}], [{65: 'there', 67: 'there'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'there', 67: 'there'}, {65: '’re', 67: 'are'}], [{65: 'there', 67: 'there'}, {65: '’ve'}], [{65: 'there', 67: 'there'}, {65: '’d', 67: \"'d\"}], [{65: 'there', 67: 'there'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'There', 67: 'there'}, {65: '’s', 67: \"'s\"}], [{65: 'There', 67: 'there'}, {65: '’ll', 67: 'will'}], [{65: 'There', 67: 'there'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'There', 67: 'there'}, {65: '’re', 67: 'are'}], [{65: 'There', 67: 'there'}, {65: '’ve'}], [{65: 'There', 67: 'there'}, {65: '’d', 67: \"'d\"}], [{65: 'There', 67: 'there'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'that', 67: 'that'}, {65: '’s', 67: \"'s\"}], [{65: 'that', 67: 'that'}, {65: '’ll', 67: 'will'}], [{65: 'that', 67: 'that'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'that', 67: 'that'}, {65: '’re', 67: 'are'}], [{65: 'that', 67: 'that'}, {65: '’ve'}], [{65: 'that', 67: 'that'}, {65: '’d', 67: \"'d\"}], [{65: 'that', 67: 'that'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'That', 67: 'that'}, {65: '’s', 67: \"'s\"}], [{65: 'That', 67: 'that'}, {65: '’ll', 67: 'will'}], [{65: 'That', 67: 'that'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'That', 67: 'that'}, {65: '’re', 67: 'are'}], [{65: 'That', 67: 'that'}, {65: '’ve'}], [{65: 'That', 67: 'that'}, {65: '’d', 67: \"'d\"}], [{65: 'That', 67: 'that'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'this', 67: 'this'}, {65: '’s', 67: \"'s\"}], [{65: 'this', 67: 'this'}, {65: '’ll', 67: 'will'}], [{65: 'this', 67: 'this'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'this', 67: 'this'}, {65: '’re', 67: 'are'}], [{65: 'this', 67: 'this'}, {65: '’ve'}], [{65: 'this', 67: 'this'}, {65: '’d', 67: \"'d\"}], [{65: 'this', 67: 'this'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'This', 67: 'this'}, {65: '’s', 67: \"'s\"}], [{65: 'This', 67: 'this'}, {65: '’ll', 67: 'will'}], [{65: 'This', 67: 'this'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'This', 67: 'this'}, {65: '’re', 67: 'are'}], [{65: 'This', 67: 'this'}, {65: '’ve'}], [{65: 'This', 67: 'this'}, {65: '’d', 67: \"'d\"}], [{65: 'This', 67: 'this'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'these', 67: 'these'}, {65: '’s', 67: \"'s\"}], [{65: 'these', 67: 'these'}, {65: '’ll', 67: 'will'}], [{65: 'these', 67: 'these'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'these', 67: 'these'}, {65: '’re', 67: 'are'}], [{65: 'these', 67: 'these'}, {65: '’ve'}], [{65: 'these', 67: 'these'}, {65: '’d', 67: \"'d\"}], [{65: 'these', 67: 'these'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'These', 67: 'these'}, {65: '’s', 67: \"'s\"}], [{65: 'These', 67: 'these'}, {65: '’ll', 67: 'will'}], [{65: 'These', 67: 'these'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'These', 67: 'these'}, {65: '’re', 67: 'are'}], [{65: 'These', 67: 'these'}, {65: '’ve'}], [{65: 'These', 67: 'these'}, {65: '’d', 67: \"'d\"}], [{65: 'These', 67: 'these'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'those', 67: 'those'}, {65: '’s', 67: \"'s\"}], [{65: 'those', 67: 'those'}, {65: '’ll', 67: 'will'}], [{65: 'those', 67: 'those'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'those', 67: 'those'}, {65: '’re', 67: 'are'}], [{65: 'those', 67: 'those'}, {65: '’ve'}], [{65: 'those', 67: 'those'}, {65: '’d', 67: \"'d\"}], [{65: 'those', 67: 'those'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'Those', 67: 'those'}, {65: '’s', 67: \"'s\"}], [{65: 'Those', 67: 'those'}, {65: '’ll', 67: 'will'}], [{65: 'Those', 67: 'those'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'Those', 67: 'those'}, {65: '’re', 67: 'are'}], [{65: 'Those', 67: 'those'}, {65: '’ve'}], [{65: 'Those', 67: 'those'}, {65: '’d', 67: \"'d\"}], [{65: 'Those', 67: 'those'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'ca', 67: 'can'}, {65: 'n’t', 67: 'not'}], [{65: 'ca', 67: 'can'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Ca', 67: 'can'}, {65: 'n’t', 67: 'not'}], [{65: 'Ca', 67: 'can'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'could', 67: 'could'}, {65: 'n’t', 67: 'not'}], [{65: 'could', 67: 'could'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Could', 67: 'could'}, {65: 'n’t', 67: 'not'}], [{65: 'Could', 67: 'could'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'do', 67: 'do'}, {65: 'n’t', 67: 'not'}], [{65: 'do', 67: 'do'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Do', 67: 'do'}, {65: 'n’t', 67: 'not'}], [{65: 'Do', 67: 'do'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'does', 67: 'does'}, {65: 'n’t', 67: 'not'}], [{65: 'does', 67: 'does'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Does', 67: 'does'}, {65: 'n’t', 67: 'not'}], [{65: 'Does', 67: 'does'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'did', 67: 'do'}, {65: 'n’t', 67: 'not'}], [{65: 'did', 67: 'do'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Did', 67: 'do'}, {65: 'n’t', 67: 'not'}], [{65: 'Did', 67: 'do'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'had', 67: 'have'}, {65: 'n’t', 67: 'not'}], [{65: 'had', 67: 'have'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Had', 67: 'have'}, {65: 'n’t', 67: 'not'}], [{65: 'Had', 67: 'have'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'may', 67: 'may'}, {65: 'n’t', 67: 'not'}], [{65: 'may', 67: 'may'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'May', 67: 'may'}, {65: 'n’t', 67: 'not'}], [{65: 'May', 67: 'may'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'might', 67: 'might'}, {65: 'n’t', 67: 'not'}], [{65: 'might', 67: 'might'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Might', 67: 'might'}, {65: 'n’t', 67: 'not'}], [{65: 'Might', 67: 'might'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'must', 67: 'must'}, {65: 'n’t', 67: 'not'}], [{65: 'must', 67: 'must'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Must', 67: 'must'}, {65: 'n’t', 67: 'not'}], [{65: 'Must', 67: 'must'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'need', 67: 'need'}, {65: 'n’t', 67: 'not'}], [{65: 'need', 67: 'need'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Need', 67: 'need'}, {65: 'n’t', 67: 'not'}], [{65: 'Need', 67: 'need'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'ought', 67: 'ought'}, {65: 'n’t', 67: 'not'}], [{65: 'ought', 67: 'ought'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Ought', 67: 'ought'}, {65: 'n’t', 67: 'not'}], [{65: 'Ought', 67: 'ought'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'sha', 67: 'shall'}, {65: 'n’t', 67: 'not'}], [{65: 'sha', 67: 'shall'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Sha', 67: 'shall'}, {65: 'n’t', 67: 'not'}], [{65: 'Sha', 67: 'shall'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'should', 67: 'should'}, {65: 'n’t', 67: 'not'}], [{65: 'should', 67: 'should'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Should', 67: 'should'}, {65: 'n’t', 67: 'not'}], [{65: 'Should', 67: 'should'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'wo', 67: 'will'}, {65: 'n’t', 67: 'not'}], [{65: 'wo', 67: 'will'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Wo', 67: 'will'}, {65: 'n’t', 67: 'not'}], [{65: 'Wo', 67: 'will'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'would', 67: 'would'}, {65: 'n’t', 67: 'not'}], [{65: 'would', 67: 'would'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Would', 67: 'would'}, {65: 'n’t', 67: 'not'}], [{65: 'Would', 67: 'would'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'could', 67: 'could'}, {65: '’ve'}], [{65: 'Could', 67: 'could'}, {65: '’ve'}], [{65: 'might', 67: 'might'}, {65: '’ve'}], [{65: 'Might', 67: 'might'}, {65: '’ve'}], [{65: 'must', 67: 'must'}, {65: '’ve'}], [{65: 'Must', 67: 'must'}, {65: '’ve'}], [{65: 'should', 67: 'should'}, {65: '’ve'}], [{65: 'Should', 67: 'should'}, {65: '’ve'}], [{65: 'would', 67: 'would'}, {65: '’ve'}], [{65: 'Would', 67: 'would'}, {65: '’ve'}], [{65: 'ai'}, {65: 'n’t', 67: 'not'}], [{65: 'Ai'}, {65: 'n’t', 67: 'not'}], [{65: 'are', 67: 'are'}, {65: 'n’t', 67: 'not'}], [{65: 'Are', 67: 'are'}, {65: 'n’t', 67: 'not'}], [{65: 'is', 67: 'is'}, {65: 'n’t', 67: 'not'}], [{65: 'Is', 67: 'is'}, {65: 'n’t', 67: 'not'}], [{65: 'was', 67: 'was'}, {65: 'n’t', 67: 'not'}], [{65: 'Was', 67: 'was'}, {65: 'n’t', 67: 'not'}], [{65: 'were', 67: 'were'}, {65: 'n’t', 67: 'not'}], [{65: 'Were', 67: 'were'}, {65: 'n’t', 67: 'not'}], [{65: 'have', 67: 'have'}, {65: 'n’t', 67: 'not'}], [{65: 'Have', 67: 'have'}, {65: 'n’t', 67: 'not'}], [{65: 'has', 67: 'has'}, {65: 'n’t', 67: 'not'}], [{65: 'Has', 67: 'has'}, {65: 'n’t', 67: 'not'}], [{65: 'dare', 67: 'dare'}, {65: 'n’t', 67: 'not'}], [{65: 'Dare', 67: 'dare'}, {65: 'n’t', 67: 'not'}], [{65: 'doin’', 67: 'doing'}], [{65: 'Doin’', 67: 'doing'}], [{65: 'goin’', 67: 'going'}], [{65: 'Goin’', 67: 'going'}], [{65: 'nothin’', 67: 'nothing'}], [{65: 'Nothin’', 67: 'nothing'}], [{65: 'nuthin’', 67: 'nothing'}], [{65: 'Nuthin’', 67: 'nothing'}], [{65: 'ol’', 67: 'old'}], [{65: 'Ol’', 67: 'old'}], [{65: 'somethin’', 67: 'something'}], [{65: 'Somethin’', 67: 'something'}], [{65: '’em', 67: 'them'}], [{65: '’ll', 67: 'will'}], [{65: '’nuff', 67: 'enough'}], [{65: 'y’', 67: 'you'}, {65: 'all'}], [{65: 'how'}, {65: '’d'}, {65: '’y', 67: 'you'}], [{65: 'How', 67: 'how'}, {65: '’d'}, {65: '’y', 67: 'you'}], [{65: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Not', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'let'}, {65: '’s', 67: 'us'}], [{65: 'Let', 67: 'let'}, {65: '’s', 67: 'us'}], [{65: 'c’m', 67: 'come'}, {65: 'on'}], [{65: 'C’m', 67: 'come'}, {65: 'on'}], [{65: '’S', 67: \"'s\"}], [{65: '’s', 67: \"'s\"}], [{65: '’re', 67: 'are'}], [{65: '’Cause', 67: 'because'}], [{65: '’cause', 67: 'because'}], [{65: '’cos', 67: 'because'}], [{65: '’Cos', 67: 'because'}], [{65: '’coz', 67: 'because'}], [{65: '’Coz', 67: 'because'}], [{65: '’cuz', 67: 'because'}], [{65: '’Cuz', 67: 'because'}], [{65: '’bout', 67: 'about'}], [{65: 'ma’am', 67: 'madam'}], [{65: 'Ma’am', 67: 'madam'}], [{65: 'o’clock', 67: \"o'clock\"}], [{65: 'O’clock', 67: \"o'clock\"}], [{65: 'lovin’', 67: 'loving'}], [{65: 'Lovin’', 67: 'loving'}], [{65: 'havin’', 67: 'having'}], [{65: 'Havin’', 67: 'having'}], [{65: '’d'}]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en.tokenizer_exceptions import TOKENIZER_EXCEPTIONS\n",
    "\n",
    "TOKENIZER_EXCEPTIONS.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "excess-rebound",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.109723Z",
     "start_time": "2021-03-28T20:32:44.104174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "a\n",
      "$\n",
      "STOCK.\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\"This is a $STOCK.\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-adaptation",
   "metadata": {},
   "source": [
    "You can add special prefixes in the form of regex by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "unlikely-croatia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.115930Z",
     "start_time": "2021-03-28T20:32:44.111815Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"tuple\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-1ac4ed0501e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcustom_prefixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefixes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\$[a-zA-Z]+\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"tuple\") to list"
     ]
    }
   ],
   "source": [
    "custom_prefixes = nlp.Defaults.prefixes + (r\"\\$[a-zA-Z]+\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-slovakia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.124436Z",
     "start_time": "2021-03-28T20:32:44.117070Z"
    }
   },
   "outputs": [],
   "source": [
    "prefix_re = spacy.util.compile_prefix_regex(custom_prefixes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-arena",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.134261Z",
     "start_time": "2021-03-28T20:32:44.125454Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "prefix_re = re.compile(r\"\\$[a-zA-Z]+\")\n",
    "tokenizer = Tokenizer(\n",
    "    nlp.vocab, prefix_search=prefix_re.search, suffix_search=suffix_re.search\n",
    ")\n",
    "\n",
    "tokens = tokenizer(\"This is a $STOCK.\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-breed",
   "metadata": {},
   "source": [
    "You can add also special-case tokenization rules. This mechanism is also used to add custom tokenizer exceptions to the language data. See the usage guide on the [languages data](https://spacy.io/usage/linguistic-features#language-data) and [tokenizer special cases](https://spacy.io/usage/linguistic-features#special-cases) for more details and examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-chester",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.146952Z",
     "start_time": "2021-03-28T20:32:44.135427Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.attrs import ORTH, NORM, LOWER\n",
    "\n",
    "dont_case = [{ORTH: \"do\"}, {ORTH: \"n't\", NORM: \"not\"}]\n",
    "gimme_case = [{ORTH: \"gi\", NORM:\"give\"}, {ORTH: \"me\", NORM: \"me\"}]\n",
    "tokenizer.add_special_case(\"don't\", dont_case)\n",
    "tokenizer.add_special_case(\"gimme\", gimme_case)\n",
    "tokens = tokenizer(\"Yo! gimme five!\")\n",
    "for token in tokens:\n",
    "    print(token.norm_)\n",
    "tokens = tokenizer(\"You don't do that\")\n",
    "for token in tokens:\n",
    "    print(token.norm_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-checklist",
   "metadata": {},
   "source": [
    "When you load a model with pretrained NER (Named Entity Recognition), like `en_core_web_sm`, it is possible to make the tokenizer to merge the token for the entities it finds. Let's check what is inside the pipeline performed by `nlp`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-jesus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.151274Z",
     "start_time": "2021-03-28T20:32:44.148001Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-consumer",
   "metadata": {},
   "source": [
    "There's a tagger, a dependency parser and the entity recognizer. Let's check the entities of the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-radar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.161196Z",
     "start_time": "2021-03-28T20:32:44.152194Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is a $1000b company.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-twenty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.166456Z",
     "start_time": "2021-03-28T20:32:44.162183Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-census",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.172302Z",
     "start_time": "2021-03-28T20:32:44.167426Z"
    }
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-working",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.200653Z",
     "start_time": "2021-03-28T20:32:44.178705Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\n",
    "    \"This is Strive School. It's worthy to merge 'Strive School' as a single token instead of two\"\n",
    ")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-mandate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.207796Z",
     "start_time": "2021-03-28T20:32:44.203763Z"
    }
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-calculator",
   "metadata": {},
   "source": [
    "Let's add \"merge_entities\" to the pipeline (you can do it only if there is the entity recognizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-military",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.212102Z",
     "start_time": "2021-03-28T20:32:44.208805Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-textbook",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.216239Z",
     "start_time": "2021-03-28T20:32:44.213031Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-montana",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.233121Z",
     "start_time": "2021-03-28T20:32:44.217083Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\n",
    "    \"This is Strive School. It's worthy to merge 'Strive School' as a single token instead of two\"\n",
    ")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-solution",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.237660Z",
     "start_time": "2021-03-28T20:32:44.234118Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXTS = [\n",
    "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
    "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-petite",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.257141Z",
     "start_time": "2021-03-28T20:32:44.238468Z"
    }
   },
   "outputs": [],
   "source": [
    "for sentence in nlp.pipe(TEXTS):\n",
    "    for token in sentence:\n",
    "        print(token)\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-maximum",
   "metadata": {},
   "source": [
    "It's also possible to merge the noun chunks into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-client",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.263274Z",
     "start_time": "2021-03-28T20:32:44.258469Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-custody",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.268793Z",
     "start_time": "2021-03-28T20:32:44.264555Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-wedding",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.282564Z",
     "start_time": "2021-03-28T20:32:44.269711Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Hello, I'm Antonio Marsella, nice to meet you.\")\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-cassette",
   "metadata": {},
   "source": [
    "## Removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-brief",
   "metadata": {},
   "source": [
    "In general, it's convenient to remove all the stop words, *i.e. very common words in a language*, because they don't help most of NLP problem such as semantic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-palace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.290058Z",
     "start_time": "2021-03-28T20:32:44.283484Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(\"Number of stop words: %d\" % len(spacy_stopwords))\n",
    "print(\"First ten stop words: %s\" % list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-tokyo",
   "metadata": {},
   "source": [
    "To remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-entry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.317300Z",
     "start_time": "2021-03-28T20:32:44.291085Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc if not token.is_stop]\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-death",
   "metadata": {},
   "source": [
    "For adding customized stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-bahamas",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:30.546200Z",
     "start_time": "2021-03-28T20:33:30.528780Z"
    }
   },
   "outputs": [],
   "source": [
    "customize_stop_words = [\"computing\", \"filtered\"]\n",
    "for w in customize_stop_words:\n",
    "    nlp.vocab[w].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-scene",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "In most natural languages, a root word can have many variants. For example, the word ‘play’ can be used as ‘playing’, ‘played’, ‘plays’, etc. You can think of similar examples (and there are plenty).\n",
    "\n",
    "**Stemming**\n",
    "\n",
    "Let’s first understand stemming:\n",
    "\n",
    "Stemming is a text normalization technique that cuts off the end or beginning of a word by taking into account a list of common prefixes or suffixes that could be found in that word\n",
    "It is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word\n",
    " \n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "Lemmatization, on the other hand, is an organized & step-by-step procedure of obtaining the root form of the word. It makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n",
    "\n",
    "Stemming algorithm works by cutting the suffix or prefix from the word. Lemmatization is a more powerful operation as it takes into consideration the morphological analysis of the word.\n",
    "\n",
    "Lemmatization returns the lemma, which is the root word of all its inflection forms.\n",
    "\n",
    "We can say that stemming is a quick and dirty method of chopping off words to its root form while on the other hand, lemmatization is an intelligent operation that uses dictionaries which are created by in-depth linguistic knowledge. Hence, Lemmatization helps in forming better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-toilet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:36.947990Z",
     "start_time": "2021-03-28T20:33:36.465998Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))\n",
    "# not using merge_chunk_nouns\n",
    "doc = nlp(\n",
    "    u\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    ")\n",
    "\n",
    "lemma_word1 = []\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        continue\n",
    "    lemma_word1.append(token.lemma_)\n",
    "lemma_word1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-large",
   "metadata": {},
   "source": [
    "## Removing the punctuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-lafayette",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:38.236544Z",
     "start_time": "2021-03-28T20:33:38.220156Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "text_no_punct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "text_no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-commerce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:38.840465Z",
     "start_time": "2021-03-28T20:33:38.808474Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text_no_punct)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-source",
   "metadata": {},
   "source": [
    "For text extracted from dialogues or chats, it is convenient to preprocess the text so that multiple occurrences of the same characters get condensed into one or two, and then use a spell checker to find the correct form of the word.\n",
    "\n",
    "A way to do that is to replace all the occurrences of repeated characters with a single one and then use a spell checker: \"hhheeelllllooo hoooowww areee youuu?\" becomes \"helo how are you?\" and then the spell checker would make it \"hello how are you?\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-cabin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:44:46.506377Z",
     "start_time": "2021-03-28T20:44:46.492492Z"
    }
   },
   "outputs": [],
   "source": [
    "st = \"hhheeeLLLLooo hoooowww areee youuu?????\"\n",
    "text = re.sub(r\"(.)\\1+\", r\"\\1\", st)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-rapid",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:44:48.622525Z",
     "start_time": "2021-03-28T20:44:48.518501Z"
    }
   },
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "text = nlp(text)\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown([token.text for token in text])\n",
    "\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-marriage",
   "metadata": {},
   "source": [
    "It didn't find any mispelled (even if there was \"helo\"). Try another spell checker:\n",
    "\n",
    "https://github.com/fsondej/autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-occupation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:43:37.953727Z",
     "start_time": "2021-03-28T20:43:37.889573Z"
    }
   },
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller()\n",
    "\n",
    "spell(text.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-language",
   "metadata": {},
   "source": [
    "As you can see, it's not always working properly! However, overall it should improve your text.\n",
    "\n",
    "If you want to create a separate lemmatizer instead of having it in the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-portrait",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T19:49:41.875502Z",
     "start_time": "2021-03-30T19:49:41.866457Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
    "\n",
    "lemmatizer = nlp.vocab.morphology.lemmatizer\n",
    "print(lemmatizer(\"studying\", VERB))\n",
    "print(lemmatizer(\"studying\", NOUN))\n",
    "print(lemmatizer(\"studying\", ADJ))\n",
    "\n",
    "# or as alternative\n",
    "\n",
    "print(lemmatizer.verb(\"studying\"))\n",
    "print(lemmatizer.noun(\"studying\"))\n",
    "print(lemmatizer.adj(\"studying\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-header",
   "metadata": {},
   "source": [
    "spaCy has no built-in stemming! However, Lemmatization is enough for most of the tasks. As alternative, you can use [NLTK library](https://www.nltk.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-showcase",
   "metadata": {},
   "source": [
    "## Part of Speech (POS) Tagging\n",
    "\n",
    "Parts of speech tagging simply refers to assigning parts of speech to individual words in a sentence, which means that, unlike phrase matching, which is performed at the sentence or multi-word level, parts of speech tagging is performed at the token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-steering",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:36:29.855279Z",
     "start_time": "2021-03-31T04:36:29.828402Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "sentence = nlp(\"Antonio is learning Python in Strive School.\")\n",
    "\n",
    "for token in sentence:\n",
    "    print(token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-chair",
   "metadata": {},
   "source": [
    "The `.pos_` attribute gives the *coarse-grained* POS tag. To inspect the *fine-grained* POS tags we could use the `.tag_`attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-beads",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:38:16.398804Z",
     "start_time": "2021-03-31T04:38:16.359022Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = nlp(\"Antonio is learning Python in Strive School.\")\n",
    "\n",
    "for token in sentence:\n",
    "    print(token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-arrow",
   "metadata": {},
   "source": [
    "While the output of the `.pos_` attribute is easy to decrypt (`PROPN`: proper noun,\n",
    "`AUX`: Auxiliary verb,\n",
    "`VERB`: verb,\n",
    "`ADP`: Adposition,\n",
    "`PUNCT`: Punctuation), the `.tag_`'s output is more cryptic. For this, you can use the `spacy.explain()` function to get the intuition behind that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-cement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:42:21.613532Z",
     "start_time": "2021-03-31T04:42:21.602635Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in sentence:\n",
    "    print(spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-mercy",
   "metadata": {},
   "source": [
    "Go and dig up your primary school grammar book!\n",
    "\n",
    "Let's put everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-bracelet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:45:40.533471Z",
     "start_time": "2021-03-31T04:45:40.515639Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in sentence:\n",
    "    print(f'{token.text:{12}} {token.pos_:{10}} {token.tag_:{8}} {spacy.explain(token.tag_)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-canberra",
   "metadata": {},
   "source": [
    "(the numbers between curly brackets define spaces for a better formatting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-greek",
   "metadata": {},
   "source": [
    "You can count the number of occurrences of each POS tag by calling the `count_by` method. \n",
    "\n",
    "The syntax is as follows (you need to pass `spacy.attrs.POS` as argument of the method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-cambodia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:01:14.881989Z",
     "start_time": "2021-03-31T05:01:14.735095Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = nlp(\"Antonio is learning Python Programming Language\")\n",
    "\n",
    "num_pos = sentence.count_by(spacy.attrs.POS)\n",
    "num_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-selection",
   "metadata": {},
   "source": [
    "The keys of the vocabulary are the ID of the POS tags, the values are their frequencies of occurrence. To retrieve the POS tags given the ID, you can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-bermuda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:05:55.476564Z",
     "start_time": "2021-03-31T05:05:55.466354Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence.vocab[96].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-military",
   "metadata": {},
   "source": [
    "where 96 is the ID of the tag. Printing all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-thursday",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:05:04.754457Z",
     "start_time": "2021-03-31T05:05:04.749748Z"
    }
   },
   "outputs": [],
   "source": [
    "for ID, frequency in num_pos.items():\n",
    "    print(f\"{ID} stands for {sentence.vocab[ID].text:{8}}: {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-chart",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case.\n",
    "\n",
    "Named entities are available as the ents property of a Doc.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-generation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:04:34.612659Z",
     "start_time": "2021-03-29T06:04:34.586397Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Antonio works at Strive School.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-swimming",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:04:49.092105Z",
     "start_time": "2021-03-29T06:04:49.075027Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-bookmark",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:06:02.488436Z",
     "start_time": "2021-03-29T06:06:02.459044Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Rome is a big city.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-reservoir",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:06:05.912053Z",
     "start_time": "2021-03-29T06:06:05.901532Z"
    }
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-gospel",
   "metadata": {},
   "source": [
    "ORG stands for organization, GPE stands for Geopolitical Entity. Some other tags are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-soldier",
   "metadata": {},
   "source": [
    "In spaCy you can list the entities by doing:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-preserve",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:54:59.282842Z",
     "start_time": "2021-03-31T05:54:59.233747Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp('Manchester United is looking to sign Harry Kane for $90 million')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-waste",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:54:59.773187Z",
     "start_time": "2021-03-31T05:54:59.766740Z"
    }
   },
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-hospital",
   "metadata": {},
   "source": [
    "We can access the entities text, label by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-lincoln",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:56:13.150302Z",
     "start_time": "2021-03-31T05:56:13.141509Z"
    }
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-giant",
   "metadata": {},
   "source": [
    "Even if the entities are self-explanatory for this example, you can use `spacy.explain()` for a detailed description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-nicaragua",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:57:32.686265Z",
     "start_time": "2021-03-31T05:57:32.678082Z"
    }
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-artist",
   "metadata": {},
   "source": [
    "### Adding new entities\n",
    "\n",
    "If in a text an entity has not being identified correctly, you can manually add it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-requirement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:08:13.466172Z",
     "start_time": "2021-03-31T06:08:13.421585Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = nlp(u'Strive is setting up a new course.')\n",
    "for entity in sentence.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-cleaning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:08:15.759623Z",
     "start_time": "2021-03-31T06:08:15.749143Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "ORG = sentence.vocab.strings[u'ORG']\n",
    "\n",
    "new_entity = Span(sentence, 0, 1, label=ORG)\n",
    "sentence.ents = list(sentence.ents) + [new_entity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-token",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:08:15.930620Z",
     "start_time": "2021-03-31T06:08:15.922601Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-problem",
   "metadata": {},
   "source": [
    "First, we need to import the Span class from the `spacy.tokens` module. Next, we need to get the hash value of the ORG entity type from our document. After that, we need to assign the hash value of ORG to the span. Since \"Strive\" is the first word in the document, the span is 0-1. Finally, we need to add the new entity span to the list of entities. Now if you execute the following script, you will see \"Strive\" in the list of entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-advantage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:10:27.513455Z",
     "start_time": "2021-03-31T06:10:27.498248Z"
    }
   },
   "outputs": [],
   "source": [
    "for entity in sentence.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-springer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:11:28.227061Z",
     "start_time": "2021-03-31T06:11:28.210463Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(sentence, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-trance",
   "metadata": {},
   "source": [
    "We can also filter which entity to display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-scenario",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:12:13.012412Z",
     "start_time": "2021-03-31T06:12:12.922949Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = nlp(u'Manchester United is looking to sign Harry Kane for $90 million. David demand 100 Million Dollars')\n",
    "displacy.render(sentence, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-buffer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:12:25.772261Z",
     "start_time": "2021-03-31T06:12:25.766031Z"
    }
   },
   "outputs": [],
   "source": [
    "filter = {'ents': ['ORG']}\n",
    "displacy.render(sentence, style='ent', jupyter=True, options=filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-toronto",
   "metadata": {},
   "source": [
    "## Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-participant",
   "metadata": {},
   "source": [
    "The Matcher lets you find words and phrases using rules describing their token attributes. Rules can refer to token annotations (like the text or part-of-speech tags), as well as lexical attributes like Token.is_punct. Applying the matcher to a Doc gives you access to the matched tokens in context. For in-depth examples and workflows for combining rules and statistical models, see the usage guide on rule-based matching.\n",
    "\n",
    "https://spacy.io/api/matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-emphasis",
   "metadata": {},
   "source": [
    "Test the explorer: https://explosion.ai/demos/matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-brake",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:48.315906Z",
     "start_time": "2021-03-31T09:15:48.312447Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-works",
   "metadata": {},
   "source": [
    "\n",
    "Let’s say we want to enable spaCy to find a combination of three tokens:\n",
    "\n",
    "- A token whose lowercase form matches “hello”, e.g. “Hello” or “HELLO”.\n",
    "- A token whose is_punct flag is set to True, i.e. any punctuation.\n",
    "- A token whose lowercase form matches “world”, e.g. “World” or “WORLD”.\n",
    "\n",
    "The pattern would be:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-biotechnology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:28:52.532218Z",
     "start_time": "2021-03-31T06:28:52.518505Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-moral",
   "metadata": {},
   "source": [
    "We can define a custom matcher that satisfy that pattern by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-collar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:29:52.821170Z",
     "start_time": "2021-03-31T06:29:52.795421Z"
    }
   },
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"HelloWorld\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-savannah",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:30:16.443906Z",
     "start_time": "2021-03-31T06:30:16.415047Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-snapshot",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:33:07.773269Z",
     "start_time": "2021-03-31T06:33:07.763583Z"
    }
   },
   "outputs": [],
   "source": [
    "golang_pattern = [{\"LOWER\":{\n",
    "    \"IN\":[\"go\", \"golang\"]}, \n",
    "                   \"POS\":{\"NOT_IN\":[\"VERB\"]}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "limited-breach",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:36:07.310668Z",
     "start_time": "2021-03-31T06:36:07.279388Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matcher' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-fa2d5fcc078d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I go the learn the Go programming language\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GOLANG\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgolang_pattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'matcher' is not defined"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I go the learn the Go programming language\")\n",
    "matcher.add(\"GOLANG\", [golang_pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-storm",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:36:12.070611Z",
     "start_time": "2021-03-31T06:36:12.066899Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-belief",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:35.197670Z",
     "start_time": "2021-03-31T09:15:35.171259Z"
    }
   },
   "outputs": [],
   "source": [
    "text = nlp(\"This is a tweet about Dogecoin $DOGE #DOGE\")\n",
    "\n",
    "for i, token in enumerate(text):\n",
    "    print(i, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-thickness",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:39.837461Z",
     "start_time": "2021-03-31T09:15:39.833204Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern = [ {\"ORTH\":\"$\", \"OP\":\"?\"},\n",
    "           {\"ORTH\":\"#\", \"OP\":\"?\"},\n",
    "        {\"LOWER\": {\"IN\":[\"doge\", \"dogecoin\"]}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-choir",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:51.783195Z",
     "start_time": "2021-03-31T09:15:51.779647Z"
    }
   },
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOGE\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-margin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:52.142087Z",
     "start_time": "2021-03-31T09:15:52.131733Z"
    }
   },
   "outputs": [],
   "source": [
    "matches = matcher(text)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = text[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-snapshot",
   "metadata": {},
   "source": [
    "You can also match \"whatever token\" by adding an empty \\{\\} in the pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-essence",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:18:49.324240Z",
     "start_time": "2021-03-31T09:18:49.304322Z"
    }
   },
   "outputs": [],
   "source": [
    "text = nlp(\"I want to match whatever follows here\")\n",
    "pattern = [ {\"LOWER\":\"follows\"},\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-violation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:18:49.728059Z",
     "start_time": "2021-03-31T09:18:49.725041Z"
    }
   },
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"WHATEVER\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-appearance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:18:49.890304Z",
     "start_time": "2021-03-31T09:18:49.882152Z"
    }
   },
   "outputs": [],
   "source": [
    "matches = matcher(text)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = text[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-peninsula",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

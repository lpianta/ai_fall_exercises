{"cells":[{"cell_type":"code","metadata":{"id":"Cin2DCEVm3jM","cell_id":"00000-d206d17c-6124-4eab-a2cc-b8777111eef7","deepnote_to_be_reexecuted":false,"source_hash":"175a437a","execution_start":1614081496875,"execution_millis":1728,"deepnote_cell_type":"code"},"source":"# Import the libraries\n\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n","execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"The initial building block of Keras is a model, and the simplest model is called `Sequential`. A sequential Keras model is a linear pipeline (a stack) of neural networks layers. This code fragment defines a single layer with 12 artificial neurons, and it expects 8 input features:\n","metadata":{"id":"uTPybHxmm3jP","cell_id":"00001-8782ccfe-c7ea-453b-9050-3632b5d4dedb","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"O035kbH-m3jQ","cell_id":"00002-b802a6bd-bff0-42db-8fc6-e41249791fbb","deepnote_to_be_reexecuted":false,"source_hash":"c9ba9a70","execution_start":1614084320910,"execution_millis":5,"deepnote_cell_type":"code"},"source":"# Create a Single Layer Perceptron in Keras\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=8, kernel_initializer='random_uniform'))\n\n","execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Each neuron can be initialized with specific weights. The most common choices provided by Keras:\n\n- `random_uniform`: Weights are initialized to uniformly random small values in (-0.05, 0.05). In other words, any value within the given interval is equally likely to be drawn.\n- `random_normal`: Weights are initialized according to a Gaussian, with a zero mean and small standard deviation of 0.05. For those of you who are not familiar with a Gaussian, think about a symmetric bell curve shape.\n- `zero`: All weights are initialized to zero.\n\n[Here](https://keras.io/initializations/) for the full list https://keras.io/initializations/ .","metadata":{"id":"9e3YdgxFm3jQ","cell_id":"00003-84f39f7a-26b7-4ff1-90a0-fb88f2341752","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Exercise 1\n\nDefine a Single Layer Perceptron in Keras with 10 as dimension of the input and 8 neurons, with only zeros as initial weights.","metadata":{"id":"xoyuPYDmm3jQ","cell_id":"00004-f5bec270-b206-434b-b8f2-40c1aedc51de","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"chiw3MsEm3jR","cell_id":"00005-69cd2ded-5765-42e9-839d-9548f30a3bc8","deepnote_to_be_reexecuted":false,"source_hash":"32b15d10","execution_start":1614084360134,"execution_millis":1,"deepnote_cell_type":"code"},"source":"# Your code here\nmodel = Sequential()\nmodel.add(Dense(8, input_dim=10, kernel_initializer='zero'))","execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"To make your model to output either 0 or 1, you have to add a line to your model:\n\n`model.add(Dense(1, activation='sigmoid'))`\n\nThe output will be consider a neuron itself with the `sigmoid` as activation function.\n\n## Exercise 1.1\n\nRewrite the Single Layer Perceptron defined in Exercise 1 so that it has the output layer.\n","metadata":{"id":"KG9TSntYm3jS","cell_id":"00006-1b900f7c-c78a-418e-ad9d-1622496f18a7","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"FwTu4U7Nm3jS","cell_id":"00007-74e70540-802d-4f15-b3e6-5186b8c24fbc","deepnote_to_be_reexecuted":false,"source_hash":"18beda4e","execution_start":1614084408745,"execution_millis":22,"deepnote_cell_type":"code"},"source":"# Your code here\nmodel = Sequential()\nmodel.add(Dense(8, input_dim=10, kernel_initializer='zero'))\nmodel.add(Dense(1, activation='sigmoid'))","execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"As always, to test your model, you need some data. However, meanwhile you can see if your model has been built correctly inspecting it with `model.summary()`.","metadata":{"id":"Z4hBaGg7m3jT","cell_id":"00008-4d004615-1a44-4886-b73f-d8bdb88bfd1e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qrbV_c6am3jU","outputId":"11056cbc-e90a-47a5-c6aa-25009afda001","cell_id":"00009-864f1058-7084-41e6-820e-0a1c6d665f50","deepnote_to_be_reexecuted":false,"source_hash":"df8a0117","execution_start":1614084420832,"execution_millis":2,"deepnote_cell_type":"code"},"source":"# Run this cell, the output should be as the one you see\nmodel.summary()","execution_count":5,"outputs":[{"name":"stdout","text":"Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_2 (Dense)              (None, 8)                 88        \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 9         \n=================================================================\nTotal params: 97\nTrainable params: 97\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now that you have successfully build a Keras model you have to *compile* it. This is where you have to define which **loss function** you want to use, which *optimizer* (not for today), and which **metrics** you want to check. \n\nWhy we need to specify these?\n- The loss function is the \"error\", defined in a certain way, that your optimizer will try to minimize by updating the weights.\n- You have already seen the accuracy, precision and recall metrics. They are used for understanding when to stop the training and to review the training process, but they are not used by the optimizer.\n\nYou can find some of the loss functions available in Keras here: https://keras.io/api/losses/\n\nSince the activation function used in the last layer is a sigmoid, it means that we are building a binary classifier, so we could use the:\n- `BinaryCrossentropy`: Computes the cross-entropy loss between true labels and predicted labels.\nUse this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction.\n\n\n","metadata":{"id":"k0TN9DPgoned","cell_id":"00010-6c855b1d-c44d-4dae-a46b-259f6e10a691","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"Tjjz70EJm3jV","cell_id":"00011-d8009392-e097-4e6b-b7c1-685a6ccb6103","deepnote_to_be_reexecuted":false,"source_hash":"c47e707a","execution_start":1614084463390,"execution_millis":1,"deepnote_cell_type":"code"},"source":"model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n","execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"If you didn't get an error, it means that your model has been successsfully compiled! Now it's time to train it! I'll give you a mock dataset to play with your model. But I want your attention here: *how should the input data look like?*\n\nLook at the input model: I asked for 10 input dimension and we output either 0 or 1. So we will have arrays of length 10 and binary labels!","metadata":{"id":"aKwpSOvBrWcN","cell_id":"00012-ad91ee6d-9e46-4135-be8f-13f98e99cfd0","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"KkqDsMyqruge","cell_id":"00013-555861b8-c4f6-47d0-9678-7d7a617b5741","deepnote_to_be_reexecuted":false,"source_hash":"b84c2f92","execution_start":1614084481727,"execution_millis":0,"deepnote_cell_type":"code"},"source":"X = np.random.rand(1000,10)\ny = np.random.randint(0,2,size=(1000,))","execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Who's your best friend? ðŸ‘‡","metadata":{"id":"zDFZyx8yte8I","cell_id":"00014-826f17f1-a27e-4ee9-8b81-0a6895e6ce0f","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"n-1SdYmctQ4-","cell_id":"00015-059a3422-6185-4405-b0aa-fac9369212c3","deepnote_to_be_reexecuted":false,"source_hash":"dd0826f","execution_start":1614084532258,"execution_millis":665,"deepnote_cell_type":"code"},"source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Now... Get **fit**, stay (mentally) healthy (since you don't have to code this from scratch anymore)!\n\nThe `fit` method is for actually training your model. So you have to define the number of `epochs` and the `batch_size`:\n\n- `epochs`: This is the number of times the model is exposed to the training set. At each iteration, the optimizer tries to adjust the weights so that the objective function is minimized.\n\n- `batch_size`: This is the number of training instances observed before the optimizer performs a weight update.\n\n\nLet's set the batch size to be 10 and the epochs 20! \n\nThe cool thing is that you can even give a percentage of the training set as validation directly in the fit!!! This means that it will automatically test the error on the validation and gives you both the training accuracy and the validation accuracy!","metadata":{"id":"Z7d7JRFft5Vb","cell_id":"00016-7fe4a312-eb9d-4a20-9a55-7135a32fff84","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"IVYA7GuAvHUs","cell_id":"00017-f7e975cb-ced9-4168-9644-4aaf346b1a97","deepnote_to_be_reexecuted":false,"source_hash":"30ef90ce","execution_start":1614084569430,"execution_millis":1,"deepnote_cell_type":"code"},"source":"EPOCHS = 20\nBATCH_SIZE = 10","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vNndDV16vO1R","outputId":"a21a73a9-daad-488d-8a23-a86159138493","cell_id":"00018-f1a3ea9a-1521-46d3-8caa-dc8b5bfa136b","deepnote_to_be_reexecuted":false,"source_hash":"dbba19db","execution_start":1614084576057,"execution_millis":3561,"deepnote_cell_type":"code"},"source":"history = model.fit(X_train, y_train,\nbatch_size=BATCH_SIZE, epochs=EPOCHS,\nverbose=1, validation_split=0.2)","execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1/20\n54/54 [==============================] - 2s 23ms/step - loss: 0.6944 - accuracy: 0.4947 - val_loss: 0.6960 - val_accuracy: 0.4104\nEpoch 2/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.4710 - val_loss: 0.6966 - val_accuracy: 0.4030\nEpoch 3/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5130 - val_loss: 0.6952 - val_accuracy: 0.4254\nEpoch 4/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6937 - accuracy: 0.5365 - val_loss: 0.6989 - val_accuracy: 0.4104\nEpoch 5/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5163 - val_loss: 0.6933 - val_accuracy: 0.5597\nEpoch 6/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.5011 - val_loss: 0.6922 - val_accuracy: 0.5522\nEpoch 7/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6911 - accuracy: 0.5578 - val_loss: 0.6938 - val_accuracy: 0.5224\nEpoch 8/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6903 - accuracy: 0.5587 - val_loss: 0.6956 - val_accuracy: 0.5075\nEpoch 9/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6912 - accuracy: 0.5083 - val_loss: 0.6951 - val_accuracy: 0.5224\nEpoch 10/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6862 - accuracy: 0.5377 - val_loss: 0.6898 - val_accuracy: 0.5597\nEpoch 11/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6888 - accuracy: 0.5431 - val_loss: 0.6900 - val_accuracy: 0.5672\nEpoch 12/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6885 - accuracy: 0.5637 - val_loss: 0.6905 - val_accuracy: 0.5672\nEpoch 13/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5284 - val_loss: 0.6934 - val_accuracy: 0.5597\nEpoch 14/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6909 - accuracy: 0.5091 - val_loss: 0.6994 - val_accuracy: 0.5000\nEpoch 15/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6827 - accuracy: 0.5613 - val_loss: 0.6928 - val_accuracy: 0.5522\nEpoch 16/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6857 - accuracy: 0.5599 - val_loss: 0.6969 - val_accuracy: 0.5224\nEpoch 17/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6899 - accuracy: 0.5254 - val_loss: 0.7011 - val_accuracy: 0.5000\nEpoch 18/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6854 - accuracy: 0.5556 - val_loss: 0.6959 - val_accuracy: 0.5373\nEpoch 19/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6877 - accuracy: 0.5455 - val_loss: 0.6889 - val_accuracy: 0.5672\nEpoch 20/20\n54/54 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5534 - val_loss: 0.6864 - val_accuracy: 0.5522\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now let's test on the test data:","metadata":{"id":"rc0NAuNHwHKP","cell_id":"00019-12dd9d4f-b898-4cc7-8288-976cdc41f9f3","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0zLbMzTvz7u","outputId":"ffe7d7df-f095-4178-9bdf-6181507efb79","cell_id":"00020-f055256a-a332-4caf-82f8-c4b7067fc648","deepnote_to_be_reexecuted":false,"source_hash":"1e8d7261","execution_start":1614084614386,"execution_millis":49,"deepnote_cell_type":"code"},"source":"score = model.evaluate(X_test, y_test, verbose=1)\nprint(f\"{model.metrics_names[0]}:\", score[0])\nprint(f\"{model.metrics_names[1]}\", score[1])","execution_count":11,"outputs":[{"name":"stdout","text":"11/11 [==============================] - 0s 3ms/step - loss: 0.6936 - accuracy: 0.4909\nloss: 0.6936046481132507\naccuracy 0.4909090995788574\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The performance is super low because we gave random data! But you can try on real data!\n\nIf you want to manually inspect the values of the prediction you can use the `predict` method:\n","metadata":{"id":"DJnYfaN5wZ9t","cell_id":"00021-2ddc6ac0-096c-4295-998c-ad1d009ed5ec","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TRITfUXgy_xJ","outputId":"72ccd4c9-a0d0-459e-bc75-98a43c659e73","cell_id":"00022-575071d0-4de0-4218-bfe4-43670abbb715","deepnote_to_be_reexecuted":false,"source_hash":"2bafb807","execution_start":1614084621549,"execution_millis":96,"deepnote_cell_type":"code"},"source":"np.random.seed(42)\npred = model.predict(np.random.rand(1,10))\npred","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"array([[0.42473602]], dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":"As you can see, the output is a \"double\" array. So if you want to get the number inside it you could access it adding `[0][0]`:","metadata":{"id":"O5GGvRbYzVU8","cell_id":"00023-c308beaa-63ec-42f6-88f0-6e578d6555f9","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"npoBW9UuzgUk","outputId":"24a1845f-f269-4daf-afb5-aed0a9c6e413","cell_id":"00024-aa15616d-d404-4627-b92f-b4f8f0cdaaa4","deepnote_to_be_reexecuted":false,"source_hash":"a88afff5","execution_start":1614084642902,"execution_millis":19,"deepnote_cell_type":"code"},"source":"pred[0][0]","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"0.42473602"},"metadata":{}}]},{"cell_type":"markdown","source":"But this is the a float! You want 0 or 1! True, you could round the prediction if you want, with threshold 0.5. Or use the predict_classes method:","metadata":{"id":"hwGpgvzx0DOp","cell_id":"00025-ebcff277-950a-47dc-ae88-b2c91dea0b20","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-WqSfPBgz6sW","outputId":"08867881-2dce-41c1-d688-9e876640ce8e","cell_id":"00026-fe63a3f7-20a9-48de-9ef7-5b1b1cfbd6d5","deepnote_to_be_reexecuted":false,"source_hash":"1ed0a20b","execution_millis":48,"execution_start":1614084667257,"deepnote_cell_type":"code"},"source":"np.random.seed(42)\n\npred_classes = model.predict_classes(np.random.rand(1,10))\npred_classes","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"array([[0]], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"Since the `predict_classes` method is deprecated, it's not convenient to use it for a maintainable code. So better to check the class manually using a threshold!","metadata":{"id":"oL6Gvqvvm3jV","cell_id":"00027-ac897fa6-59e2-4600-9418-476fdd3172fb","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# BIGGER NETWORKS\n\nYou can add as many layers you(r RAM) want(s) in your neural network! Before we used only one! It's as simple as adding \n\n`model.add(Dense(N_HIDDEN))`\n\nbetween the the layers!","metadata":{"id":"aPS8OFe21NVE","cell_id":"00028-e813b434-97fe-4ee9-9ef0-7f0826e36673","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"lUr6kXjf1LXt","cell_id":"00029-52c148b9-126b-4ea0-bd00-e25533ff6bdf","deepnote_to_be_reexecuted":false,"source_hash":"22a54c7f","execution_start":1614084669501,"execution_millis":30,"deepnote_cell_type":"code"},"source":"model = Sequential()\nmodel.add(Dense(12, input_dim=8, kernel_initializer='random_uniform'))\nmodel.add(Dense(100))\nmodel.add(Dense(1, activation='sigmoid'))\n\n","execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oSliT7qD2aJ9","outputId":"534c291d-f026-49a7-80ca-bcc00b1be830","cell_id":"00030-034eaf87-5da9-40d6-b2fb-425eea3cfd71","deepnote_to_be_reexecuted":false,"source_hash":"4e6a3b95","execution_start":1614084674042,"execution_millis":5,"deepnote_cell_type":"code"},"source":"model.summary()","execution_count":17,"outputs":[{"name":"stdout","text":"Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_4 (Dense)              (None, 12)                108       \n_________________________________________________________________\ndense_5 (Dense)              (None, 100)               1300      \n_________________________________________________________________\ndense_6 (Dense)              (None, 1)                 101       \n=================================================================\nTotal params: 1,509\nTrainable params: 1,509\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The rest, is exactly the same of before!\n\n## Now it's your turn!\n\nI'll give you some input data, and I want you to create a neural network with:\n\n- `input_dim` : adeguate to fit the data I will provide you\n- 32 neurons in the input layer\n- 64 neurons in the first hidden layer\n- 32 neuron in the second hidden layer\n- a binary output layer with the sigmoid\n\nYou're free to choose the rest of the parameters!","metadata":{"id":"3yuEn4pQ2kNo","cell_id":"00031-66ac2083-4f32-424a-9038-a523a31504bf","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"f7YzNg0r3P6z","cell_id":"00032-5a7708c1-7923-4497-9b50-1c357d99c3bd","deepnote_cell_type":"code"},"source":"np.random.seed(42)\nX = np.random.rand(1000, 20)\ny = np.random.randint(0, 2, size=1000)","execution_count":77,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vENjpH2e3M_N","outputId":"53cc6f28-9d16-4ee2-cf17-4df9d23e1a03","cell_id":"00033-c8e0c29e-f01c-4578-af58-e06fadc6bc85","deepnote_to_be_reexecuted":false,"source_hash":"cbc6af61","execution_millis":5,"execution_start":1614085403006,"deepnote_cell_type":"code"},"source":"# Your code here\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=10, kernel_initializer='random_normal'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()\n\n","execution_count":30,"outputs":[{"name":"stdout","text":"Model: \"sequential_7\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_19 (Dense)             (None, 32)                352       \n_________________________________________________________________\ndense_20 (Dense)             (None, 64)                2112      \n_________________________________________________________________\ndense_21 (Dense)             (None, 32)                2080      \n_________________________________________________________________\ndense_22 (Dense)             (None, 1)                 33        \n=================================================================\nTotal params: 4,577\nTrainable params: 4,577\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n","metadata":{"tags":[],"cell_id":"00034-5692dc02-622f-4363-a46f-448aff3a3304","deepnote_to_be_reexecuted":false,"source_hash":"c47e707a","execution_millis":4,"execution_start":1614085414380,"deepnote_cell_type":"code"},"outputs":[],"execution_count":32},{"cell_type":"code","metadata":{"id":"QKvHjTh43tPr","cell_id":"00034-c1305423-f888-44c2-bc8e-f9f29db04351","deepnote_to_be_reexecuted":false,"source_hash":"4288453a","execution_millis":3,"execution_start":1614085314447,"deepnote_cell_type":"code"},"source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":26,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"tags":[],"cell_id":"00036-8e21f42e-3c73-459d-9bcd-b4168ed44713","deepnote_to_be_reexecuted":false,"source_hash":"38f368cd","execution_millis":7,"execution_start":1614085387754,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"(670, 10)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"history1 = model.fit(X_train, y_train,\nbatch_size=34, epochs=50,\nverbose=1, validation_split=0.2)","metadata":{"tags":[],"cell_id":"00035-50de5ed6-2f37-44f1-b8b1-809941c95510","deepnote_to_be_reexecuted":false,"source_hash":"ad9df5c7","execution_millis":3408,"execution_start":1614085416448,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epoch 1/50\n16/16 [==============================] - 1s 11ms/step - loss: 0.6941 - accuracy: 0.5214 - val_loss: 0.6908 - val_accuracy: 0.5522\nEpoch 2/50\n16/16 [==============================] - 0s 5ms/step - loss: 0.6917 - accuracy: 0.5493 - val_loss: 0.6926 - val_accuracy: 0.5448\nEpoch 3/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5481 - val_loss: 0.6926 - val_accuracy: 0.5373\nEpoch 4/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6887 - accuracy: 0.5961 - val_loss: 0.6924 - val_accuracy: 0.5597\nEpoch 5/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6900 - accuracy: 0.5581 - val_loss: 0.6963 - val_accuracy: 0.5373\nEpoch 6/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.5432 - val_loss: 0.6926 - val_accuracy: 0.5522\nEpoch 7/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.5401 - val_loss: 0.6879 - val_accuracy: 0.5597\nEpoch 8/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6848 - accuracy: 0.5746 - val_loss: 0.6942 - val_accuracy: 0.5299\nEpoch 9/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5329 - val_loss: 0.6863 - val_accuracy: 0.5672\nEpoch 10/50\n16/16 [==============================] - 0s 4ms/step - loss: 0.6872 - accuracy: 0.5593 - val_loss: 0.6944 - val_accuracy: 0.5597\nEpoch 11/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6862 - accuracy: 0.5368 - val_loss: 0.6849 - val_accuracy: 0.5522\nEpoch 12/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6919 - accuracy: 0.5280 - val_loss: 0.6939 - val_accuracy: 0.5597\nEpoch 13/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6798 - accuracy: 0.5999 - val_loss: 0.6935 - val_accuracy: 0.5597\nEpoch 14/50\n16/16 [==============================] - 0s 4ms/step - loss: 0.6811 - accuracy: 0.5701 - val_loss: 0.7018 - val_accuracy: 0.5448\nEpoch 15/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6829 - accuracy: 0.5750 - val_loss: 0.6937 - val_accuracy: 0.5522\nEpoch 16/50\n16/16 [==============================] - 0s 4ms/step - loss: 0.6853 - accuracy: 0.5770 - val_loss: 0.6895 - val_accuracy: 0.5448\nEpoch 17/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6824 - accuracy: 0.5572 - val_loss: 0.6951 - val_accuracy: 0.5373\nEpoch 18/50\n16/16 [==============================] - 0s 4ms/step - loss: 0.6847 - accuracy: 0.5768 - val_loss: 0.6919 - val_accuracy: 0.5522\nEpoch 19/50\n16/16 [==============================] - 0s 4ms/step - loss: 0.6827 - accuracy: 0.5696 - val_loss: 0.6918 - val_accuracy: 0.5597\nEpoch 20/50\n16/16 [==============================] - 0s 4ms/step - loss: 0.6834 - accuracy: 0.5672 - val_loss: 0.6857 - val_accuracy: 0.5672\nEpoch 21/50\n16/16 [==============================] - 0s 4ms/step - loss: 0.6764 - accuracy: 0.5827 - val_loss: 0.6851 - val_accuracy: 0.5746\nEpoch 22/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6784 - accuracy: 0.5917 - val_loss: 0.7094 - val_accuracy: 0.5224\nEpoch 23/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6723 - accuracy: 0.5610 - val_loss: 0.6909 - val_accuracy: 0.5448\nEpoch 24/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6748 - accuracy: 0.5939 - val_loss: 0.6954 - val_accuracy: 0.5522\nEpoch 25/50\n16/16 [==============================] - 0s 5ms/step - loss: 0.6804 - accuracy: 0.5798 - val_loss: 0.6980 - val_accuracy: 0.5522\nEpoch 26/50\n16/16 [==============================] - 0s 4ms/step - loss: 0.6815 - accuracy: 0.5904 - val_loss: 0.7030 - val_accuracy: 0.5672\nEpoch 27/50\n16/16 [==============================] - 0s 4ms/step - loss: 0.6748 - accuracy: 0.5701 - val_loss: 0.6961 - val_accuracy: 0.5448\nEpoch 28/50\n16/16 [==============================] - 0s 4ms/step - loss: 0.6753 - accuracy: 0.6171 - val_loss: 0.6937 - val_accuracy: 0.5522\nEpoch 29/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6807 - accuracy: 0.5488 - val_loss: 0.6928 - val_accuracy: 0.5448\nEpoch 30/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6711 - accuracy: 0.6296 - val_loss: 0.6941 - val_accuracy: 0.5448\nEpoch 31/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6785 - accuracy: 0.5974 - val_loss: 0.7065 - val_accuracy: 0.5299\nEpoch 32/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6747 - accuracy: 0.5886 - val_loss: 0.6919 - val_accuracy: 0.5746\nEpoch 33/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6705 - accuracy: 0.5734 - val_loss: 0.6911 - val_accuracy: 0.5597\nEpoch 34/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6804 - accuracy: 0.5671 - val_loss: 0.6924 - val_accuracy: 0.5672\nEpoch 35/50\n16/16 [==============================] - 0s 5ms/step - loss: 0.6678 - accuracy: 0.5893 - val_loss: 0.7104 - val_accuracy: 0.5149\nEpoch 36/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6275 - val_loss: 0.7123 - val_accuracy: 0.5448\nEpoch 37/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6754 - accuracy: 0.5703 - val_loss: 0.6901 - val_accuracy: 0.5672\nEpoch 38/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6693 - accuracy: 0.6114 - val_loss: 0.7026 - val_accuracy: 0.5299\nEpoch 39/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6651 - accuracy: 0.5891 - val_loss: 0.7008 - val_accuracy: 0.5299\nEpoch 40/50\n16/16 [==============================] - 0s 4ms/step - loss: 0.6781 - accuracy: 0.5730 - val_loss: 0.6948 - val_accuracy: 0.5522\nEpoch 41/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6681 - accuracy: 0.6069 - val_loss: 0.6910 - val_accuracy: 0.5448\nEpoch 42/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6602 - accuracy: 0.6131 - val_loss: 0.6982 - val_accuracy: 0.5299\nEpoch 43/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6634 - accuracy: 0.6100 - val_loss: 0.6992 - val_accuracy: 0.5299\nEpoch 44/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6672 - accuracy: 0.6052 - val_loss: 0.7019 - val_accuracy: 0.5522\nEpoch 45/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6539 - accuracy: 0.6123 - val_loss: 0.7068 - val_accuracy: 0.5000\nEpoch 46/50\n16/16 [==============================] - 0s 4ms/step - loss: 0.6523 - accuracy: 0.6222 - val_loss: 0.6949 - val_accuracy: 0.5373\nEpoch 47/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6599 - accuracy: 0.6244 - val_loss: 0.6983 - val_accuracy: 0.5299\nEpoch 48/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6600 - accuracy: 0.6283 - val_loss: 0.6997 - val_accuracy: 0.5000\nEpoch 49/50\n16/16 [==============================] - 0s 3ms/step - loss: 0.6649 - accuracy: 0.5957 - val_loss: 0.6962 - val_accuracy: 0.5448\nEpoch 50/50\n16/16 [==============================] - 0s 4ms/step - loss: 0.6442 - accuracy: 0.6285 - val_loss: 0.6895 - val_accuracy: 0.5597\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=f8d4ffbe-6f85-4e4c-ba55-442ed6e8dcc8' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Easy Keras Exercises.ipynb","provenance":[]},"deepnote_notebook_id":"11047a0c-51ad-4618-ba51-9fad7213e2f9","deepnote":{},"deepnote_execution_queue":[]}}